{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1gV3JlG5aV_aBBIQDplrE6Ks_j6zgrxcU","timestamp":1705175890992}],"gpuType":"T4","authorship_tag":"ABX9TyPI++hE/TfDSVq93Wfc2xRd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# CNNs: Better classifiers with Transfer Learning\n","\n","This notebook how we can use transfer learning to obtain better neural networks.\n","\n","In practice, very few people train an entire CNN from scratch (with random initialization), because it is relatively rare to have a dataset of sufficient size. Instead, it is common to pretrain a ConvNet on a very large dataset (e.g. ImageNet, which contains 1.2 million images with 1000 categories), and then use the ConvNet either as an initialization or a fixed feature extractor for the task of interest.\n","\n","**The two major transfer learning scenarios look as follows:**\n","\n","- **Finetuning the ConvNet:** Instead of random initialization, we initialize the network with a pretrained network, like the one that is trained on imagenet 1000 dataset. Rest of the training looks as usual.\n","- **ConvNet as fixed feature extractor:** Here, we will freeze the weights for all of the network except that of the final fully connected layer. This last fully connected layer is replaced with a new one with random weights and only this layer is trained."],"metadata":{"id":"wQPxrvIF6Xau"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"uVMmGMr45hfW"},"outputs":[],"source":["import torchvision\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","import torch.utils.data as data\n","import torch.nn as nn\n","import torch.optim as optim\n","from tqdm import tqdm, trange\n","import torchvision.transforms as transforms\n","import time"]},{"cell_type":"code","source":["!pip install pytorch-ignite"],"metadata":{"id":"Nm8G2Y414dzb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from ignite.handlers import FastaiLRFinder\n","from ignite.engine import create_supervised_trainer"],"metadata":{"id":"6bzge6sv4ej_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["DEVICE = 'cuda'\n","BATCH_SIZE = 64"],"metadata":{"id":"AMqcfTE5NKh7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Helper functions"],"metadata":{"id":"2qkAkS2pTVIb"}},{"cell_type":"code","source":["def plot_accuracy(stats, title=None):\n","\n","  epochs = [item['epoch'] for item in stats]\n","  train_accs = [item['train_acc'] for item in stats]\n","  val_accs = [item['val_acc'] for item in stats]\n","\n","  plt.plot(epochs, train_accs, label='Train Acc')\n","  plt.plot(epochs, val_accs, label=f'Val Acc [Best: {max(val_accs):.2f}%]')\n","  plt.legend()\n","\n","  if title:\n","    plt.title(title)"],"metadata":{"id":"j3QFx1X948LG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def eval_accuracy(model, loader):\n","\n","    '''\n","    Measure the accuracy of the given model on the provided dataloader\n","    '''\n","\n","    epoch_acc = 0\n","\n","    model.eval()\n","\n","    num_corr_pred = 0\n","    num_total_pred = 0\n","\n","    with torch.no_grad():\n","\n","        for x, y in loader:\n","\n","            x = x.to(DEVICE)\n","            y = y.to(DEVICE)\n","\n","            y_pred = model(x)\n","\n","            top_pred = y_pred.argmax(1)\n","            num_corr_pred += (top_pred == y).sum()\n","            num_total_pred += len(y)\n","\n","    acc = num_corr_pred / num_total_pred * 100\n","\n","    return acc.item()"],"metadata":{"id":"0P_6IUAeMrUb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def train(model, train_loader, optimizer, criterion, num_epochs, val_loader=None):\n","\n","  '''\n","  Trains the model on the dataloader for a given number of epochs\n","   '''\n","\n","  print('===== Start training ===== \\n')\n","\n","  model.train()\n","\n","  start = time.time()\n","\n","  stats = []\n","\n","  for epoch in range(1, num_epochs+1):\n","\n","    epoch_loss = 0\n","\n","    for x, y in train_loader:\n","\n","      optimizer.zero_grad()\n","\n","      x = x.to(DEVICE)\n","      y = y.to(DEVICE)\n","\n","      y_pred = model(x)\n","\n","      loss = criterion(y_pred, y)\n","\n","      loss.backward()\n","      optimizer.step()\n","\n","      epoch_loss += loss.item()\n","\n","    print(f'[{epoch}] Loss: {epoch_loss:.3f}')\n","\n","    if epoch % 5 == 0:\n","      train_acc = eval_accuracy(model, train_loader)\n","      if val_loader:\n","        val_acc = eval_accuracy(model, val_loader)\n","        print(f'[{epoch}] Train Acc: {train_acc:.2f}%  /  Val Acc: {val_acc:.2f}%')\n","        stats.append({'epoch': epoch, 'train_acc': train_acc, 'val_acc': val_acc})\n","      else:\n","        print(f'[{epoch}] Train Acc: {train_acc:.2f}%')\n","        stats.append({'epoch': epoch, 'train_acc': train_acc})\n","\n","\n","  end = time.time()\n","  elapsed_time = end - start\n","\n","  print()\n","  print('===== Finished training ===== ')\n","  print(f'Elapsed time in minutes: {elapsed_time/60:.2f}')\n","\n","  return stats\n"],"metadata":{"id":"UAQJWPMYMlN_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Prepare the datasets and dataloaders\n","\n","The Flowers102 dataset is composed of three subsets (train, val and test). <br/>\n","In this code block we prepare the dataloaders for those three subsets."],"metadata":{"id":"aYFh_T_9Sq83"}},{"cell_type":"code","source":["# TODO Prepare the transformation and datasets"],"metadata":{"id":"hruo8RF75l2k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_loader = data.DataLoader(train_dataset, shuffle=True, batch_size=BATCH_SIZE)\n","val_loader = data.DataLoader(val_dataset, shuffle=False, batch_size=BATCH_SIZE)\n","test_loader = data.DataLoader(test_dataset, shuffle=False, batch_size=BATCH_SIZE)"],"metadata":{"id":"ClLh2aeF5oTW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ConvNet as fixed feature extractor"],"metadata":{"id":"e70jYwF9_E07"}},{"cell_type":"markdown","source":["### Prepare the pretrained Alex model\n","\n"],"metadata":{"id":"vzf6_-cvPv1h"}},{"cell_type":"markdown","source":["First, let's download a pretrained AlexNet model from PyTorch Hub. Like most CNN models, the model was trained on the ImageNet dataset which has 1000 classes."],"metadata":{"id":"t0u3G0bH_lK-"}},{"cell_type":"code","source":["# TODO: Load a pretrained AlexNet model"],"metadata":{"id":"wxELPQK95pSJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In the first part of this tutorial, our goal will be to train the classification head but keep the weights of the feature extractor fixed. To accomplish this, we first have to expect how we can access the classification head and the feature extractor in the model.\n","\n","As can be seen below, the classification head is referred to as \"classifier\" and the feature extractor is referred to as \"features\"."],"metadata":{"id":"0TWWuxJMAGLK"}},{"cell_type":"code","source":["# TODO: Check the architecture of the model"],"metadata":{"id":"InRsilg7c_CB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As the network outputs a 1000-dimensional vector, we have to modify the final linear layer to output 102 classes."],"metadata":{"id":"agut-EZOBouP"}},{"cell_type":"code","source":["# TODO: Modify the classification head"],"metadata":{"id":"47pHPrfSc9ZG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Furthermore, to reduce the memory footprint and speed-up training, we will disable the gradien computation for the weights in the feature extraction head. As we don't want to update these weights, we also don't need their gradients."],"metadata":{"id":"jcDTBy-6B_pC"}},{"cell_type":"code","source":["# TODO: Disable gradient computation for the feature extractor"],"metadata":{"id":"-A0c8A9RbGcV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### LR range test"],"metadata":{"id":"lKJ8-qHx4mcD"}},{"cell_type":"code","source":["start_lr = 1e-7\n","end_lr = 1e+1\n","optimizer = optim.Adam(model.classifier.parameters(), lr=1e-3)\n","criterion = nn.CrossEntropyLoss()\n","trainer = create_supervised_trainer(model, optimizer, criterion, DEVICE)\n","\n","lr_finder = FastaiLRFinder()\n","to_save = {\"model\": model, \"optimizer\": optimizer}\n","\n","with lr_finder.attach(trainer, to_save=to_save, start_lr=start_lr, end_lr=end_lr, num_iter=200) as trainer_with_lr_finder:\n","    trainer_with_lr_finder.run(train_loader)\n","\n","# Get lr_finder results\n","lr_finder.get_results()\n","\n","# Plot lr_finder results (requires matplotlib)\n","lr_finder.plot()\n","\n","# get lr_finder suggestion for lr\n","lr_finder.lr_suggestion()"],"metadata":{"id":"85nC0zWd4oaC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Train the model"],"metadata":{"id":"tqrkVtknR9rX"}},{"cell_type":"code","source":["num_epochs = 50\n","\n","# Important: Configure the optimizer to only update the weights of the classifier\n","optimizer = optim.Adam(model.classifier.parameters(), lr=1e-4)\n","criterion = nn.CrossEntropyLoss()"],"metadata":{"id":"z8hBY7d06Oww"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["stats = train(model, train_loader, optimizer, criterion, num_epochs, val_loader)"],"metadata":{"id":"sJNv6pNc6JF_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Plot accuracy\n","\n"],"metadata":{"id":"4YeF2SayQYIk"}},{"cell_type":"code","source":["plot_accuracy(stats, title='Pretrained AlexNet - Finetune classifier [With augmentation]')"],"metadata":{"id":"SgVcNOso7vpR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_acc = eval_accuracy(model, test_loader)\n","print(f'Accuracy on the test set (final model): {test_acc:.2f}%')"],"metadata":{"id":"oYqarVVM54aZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Finetune the entire Conv Net\n","\n","The following cells show how to train the entire conv net. The training procedure is similar to training a neural network from scratch. The only difference is that pretrained weights are used."],"metadata":{"id":"NWZN_tsIDfWo"}},{"cell_type":"code","source":["# TODO: Load the pretrained AlexNet model\n","# TODO: Modify the final layer"],"metadata":{"id":"QK1kavD20cr-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = model.to(DEVICE)"],"metadata":{"id":"_kd7bC8c0sqy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["num_epochs = 100\n","optimizer = optim.Adam(model.parameters(), lr=1e-4)\n","criterion = nn.CrossEntropyLoss()"],"metadata":{"id":"IRD_GZBm0us-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["stats = train(model, train_loader, optimizer, criterion, num_epochs, val_loader)"],"metadata":{"id":"dOcFgrFV0wZR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plot_accuracy(stats, title='Pretrained AlexNet - Entire network [With augmentation]')"],"metadata":{"id":"rP6rOjz9AlPD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["test_acc = eval_accuracy(model, test_loader)\n","print(f'Accuracy on the test set (final model): {test_acc:.2f}%')"],"metadata":{"id":"8V4vZDktAsgv"},"execution_count":null,"outputs":[]}]}