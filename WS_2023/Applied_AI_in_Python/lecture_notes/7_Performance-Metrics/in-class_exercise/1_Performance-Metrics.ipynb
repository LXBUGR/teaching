{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccd68537",
   "metadata": {},
   "source": [
    "# How do you evaluate the performance of classification models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e633321",
   "metadata": {},
   "source": [
    "So far, the only error metric that we used was the RMSE (Root Mean Square Error) to measure the performance of the regression models applied in the context of the California house pricing problem. In this notebook, we will take a closer look at performance metrics for classification models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c8e326",
   "metadata": {},
   "source": [
    "## Accuracy: How often is my model correct?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef429fad",
   "metadata": {},
   "source": [
    "Accuracy is probably the most commonly employed metric to evaluate the performance of a classification model. The accuracy of a model is the percentage of times that it is correct. In other words, it is the **ratio between the number of correctly predicted data points and the total number of data points**.\n",
    "\n",
    "For example, if we evaluate a model on a test dataset of 1000 samples and the model predicts the correct label of the samples 875 times, then this model has an accuracy of 875/1000 = 0.875, or 87.5%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0a2e13",
   "metadata": {},
   "source": [
    "#### The problem with accuracy\n",
    "\n",
    "Let's now assume that we have a medical dataset with samples from 1000 patients. Out of them, 10 have been diagnosed as sick, and the remaining 990 have been diagnosed as healthy. Our goal is to predict the diagnosis based on the features of each patient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9260b43",
   "metadata": {},
   "source": [
    "Let's further assume that we trained a model and the model predicts that all of the patients are healthy. What is the accuracy of the model?\n",
    "\n",
    "Well, let's see ... 990/1000 = 99\\%. The accuracy of the model is 99\\%! <br/> \n",
    "Although the model is pretty useless (it hasn't identified any of the sick patients), it performs almost perfectly according to the accuracy. This showcases that accuracy is not always a good metric to measure the performance of a model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3052015",
   "metadata": {},
   "source": [
    "**What would be a better metric?** Instead of measuring the accuracy, we could, for example, measure how many of the sick patients in the dataset were correctly diagnosed as sick by the model. The ratio between these two groups is also referred to as *recall*. In our example, the recall is 0, which clearly indicates the model is bad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b9dd41",
   "metadata": {},
   "source": [
    "## False positives and false negatives: Which one is worse?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a71904",
   "metadata": {},
   "source": [
    "In many cases, the total number of errors doesn't tell us everything about the model's performance,\n",
    "and we need to dig in deeper and identify certain types of errors in different ways. What are the two types of errors that the model in our example can make? It can\n",
    "diagnose a healthy person as sick or a sick person as healthy. In our model, we label the sick patients\n",
    "as positive. \n",
    "\n",
    "Consequently, the two error types are called false positives and negatives, are defined as follows:\n",
    "    \n",
    "- **False positive (FP)**: a healthy person who is incorrectly diagnosed as sick\n",
    "- **False negative (FN)**: a sick person who is incorrectly diagnosed as healthy\n",
    "    \n",
    "Naturally, the cases that are correctly diagnosed are named as follows:\n",
    "    \n",
    "- **True positive (TP)**: a sick person who is diagnosed as sick\n",
    "- **True negative (TN)**: a healthy person who is diagnosed as healthy\n",
    "\n",
    "Note that the number of positive samples $P=TP+FN$ and the number of negative samples $N=TN+FP$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d5323e",
   "metadata": {},
   "source": [
    "The question that remains is which of the two errors, FP or FN, is worse. Is it worse to incorrectly diagnose a healthy patient as sick or a sick patient as healthy? Well, incorrectly diagnosing a patient as healthy means that the patient won't get any medical treatment, it probably means that false negatives are much worse. If we incorrectly diagnose a patient as being sick, we can still ask him/her to do a second test. \n",
    "\n",
    "On the other hand, if we want to build a spam detector, incorrectly classifying an email as spam (positive) is much worse than failing to classify an email as spam. Therefore, having many false positives is much worse. \n",
    "\n",
    "As can be seen in these examples, it always depends on the problem what type of error is more important, and we therefore should focus on it. Accuracy treats both types of errors as equally important, which might not always be ideal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "723f344b",
   "metadata": {},
   "source": [
    "## Confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0e4779",
   "metadata": {},
   "source": [
    "A common way to show FP, FN, TP and TN is in the form of a matrix. We refer to this representation as a **confusion matrix**. For binary classification models (models that predict two classes), the confusion matrix has two rows and two columns. Note that the elements on the diagonal are classified correctly, and the elements off the diagonal are not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eead5cd",
   "metadata": {},
   "source": [
    "|              | Predicted Positive | Predicted Negative |\n",
    "|:------------:|:------------------:|:------------------:|\n",
    "| **Positive** |   TP               |     FN             |\n",
    "| **Negative** |   FP               |     TN             |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68fb45ce",
   "metadata": {},
   "source": [
    "For problems with more classes, we have a larger confusion matrix. For example, if our model classifies images into three classes, then our confusion matrix is a three-by-three matrix, where along the rows we have the true labels, and along the columns we have the predicted labels. This confusion matrix also has the property that the correctly classified samples are counted on the diagonal, and the incorrectly classified samples are counted off the diagonal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8ffbf5",
   "metadata": {},
   "source": [
    "|              | Predicted Class 1   | Predicted Class 2    | Predicted Class 3 |\n",
    "|:------------:|:-------------------:|:--------------------:|:---------------------:|\n",
    "| **Class 1**  |   ...     |     ... | ... |\n",
    "| **Class 2**  |   ...     |     ... | ... |\n",
    "| **Class 3**  |   ...     |     ... | ... |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc5e86d",
   "metadata": {},
   "source": [
    "## Recall: Among the positive examples, how many did we correctly classify?\n",
    "\n",
    "Building upon the insight that FNs are more important in the case of our medical dataset, we need a metric that takes this fact into account. A metric that we can use is **recall**, which answers the question of how many of the positive samples are positive.\n",
    "\n",
    "$Recall = \\frac{TP}{P} = \\frac{TP}{TP+FN} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a31f73",
   "metadata": {},
   "source": [
    "Assuming that our model always predicts that the patient is healthy, the number of TP is 0 and the number of FN is 10. Hence, the recall of our model is $\\frac{0}{0+10}=0.$ Consequently, we can conclude that our model is extremely bad if detecting positives (sick patients) is important.\n",
    "\n",
    "**Remark:** Recall is also referred to as sensitivity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940afc75",
   "metadata": {},
   "source": [
    "## Precision: Among the examples we classified as positive, how many did we correctly classify?\n",
    "\n",
    "If we care about the number of false positives using recall as a performance metric is a bad choice as it doesn't take the number of false positives into account at all. To see this, simply assume that our model always predicts that a patient is sick. In this case, the recall of the model would be $\\frac{10}{10+0}=1$ which indicates that the model is perfect. However, the number of false positives is 990!\n",
    "\n",
    "A metric that focuses on the number of false positives is **precision**. It answers the question of how many of the samples we classified as positive are positive.\n",
    "\n",
    "$Precision = \\frac{TP}{TP+FP}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ca27eb",
   "metadata": {},
   "source": [
    "## F-Score: Combining precision and recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77df444",
   "metadata": {},
   "source": [
    "We have learned about two metrics, **precision and recall**, which **focus on either false positives or false negatives**. However, in real life, both are important. However, they are important to different degrees depending on the problem. \n",
    "\n",
    "For example, we may want a model that doesn't misdiagnose any sick person but also doesn't misdiagnose too many healthy people, because misdiagnosing a healthy person may involve unnecessary and painful testing, or even an unnecessary surgery, which could affect their health negatively. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00d4213",
   "metadata": {},
   "source": [
    "The most obvious way to combine both metrics would be to compute the average between precision and recall. A good model is one that has good recall and good precision. If a model has, say, recall of 50% and precision of 100%, the\n",
    "average is 75%. This is a good score, but the model may not be, because a recall of 50% is not very\n",
    "good. We need a metric that behaves like the average but is closer to the minimum value of the two. \n",
    "\n",
    "A quantity that satisfies this property is the *harmonic mean*. The harmonic mean between two numbers $a$ and $b$ is $2ab/(a+b)$. Note that the harmonic mean is always smaller than or equal to the average. If the numbers $a$ and $b$\n",
    "are equal, one can quickly check that their harmonic mean is equal to both of them, just like the\n",
    "average. But in other cases, the harmonic mean is smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d82900c",
   "metadata": {},
   "source": [
    "The $F_1$-score is defined as the harmonic mean between precision $P$ and recall $R$.\n",
    "\n",
    "$F_1 = \\frac{2PR}{P+R}$\n",
    "\n",
    "We use the $F_1$-score if precision and recall are considered equally important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6acaffb",
   "metadata": {},
   "source": [
    "But what if we want to express that either precision or recall is more important? For example, in the case of our medical dataset, we hypothesized that recall should be more important than precision. To incorporate this information into our formula, we want to add more weight to the precision.\n",
    "\n",
    "This leads to what is referred to as $F_\\beta$-score. The $F_\\beta$ -score is defined as follows:\n",
    "\n",
    "$F_\\beta = \\frac{(1+\\beta^2)PR}{\\beta^2 P+R}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac22831a",
   "metadata": {},
   "source": [
    "Let's a few different values of $\\beta$ to see how it effects the $F_\\beta$ score."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4677dca9",
   "metadata": {},
   "source": [
    "**Case $\\beta=1$**:\n",
    "    \n",
    "$F_1= \\frac{(1+1^2)PR}{1^2 P+R} = \\frac{2PR}{P+R}$\n",
    "\n",
    "$\\rightarrow$ Like $F_1$ score. Recall and precision are considered equally important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a79229",
   "metadata": {},
   "source": [
    "**Case $\\beta=0$**:\n",
    "    \n",
    "$F_0= \\frac{(1+0^2)PR}{0^2 P+R} = \\frac{PR}{R} = P$\n",
    "\n",
    "$\\rightarrow$ Recall is ignored. We only focus on precision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7f19b7",
   "metadata": {},
   "source": [
    "**Case $\\beta=\\infty$**:\n",
    "    \n",
    "$F_\\infty = \\lim \\limits_{\\beta \\to \\infty} \\frac{(1+\\beta^2)PR}{\\beta^2 P+R} = \\lim \\limits_{\\beta \\to \\infty} \\frac{(\\frac{1}{\\beta^2}+1)PR}{P+\\frac{1}{\\beta^2}R} = \\frac{PR}{P} = R$\n",
    "\n",
    "$\\rightarrow$ Precision is ignored. We only focus on recall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b1cd41",
   "metadata": {},
   "source": [
    "### Which value should we choose for $\\beta$?\n",
    "\n",
    "Obviously, the choice of $\\beta$ depends on whether we want to give precision or recall more importance. In practice, a common choice is $\\beta=2$ if recall should be weighted higher than precision, $\\beta=0.5$ if precision should be weighted higher than recall, or $\\beta=0$ if both are equally important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fae3fdd",
   "metadata": {},
   "source": [
    "### Problems with imbalanced classes\n",
    "\n",
    "It's important to be aware that the F-beta-score is dependent on the ratio of positive and negative samples. This means that the comparison of F-beta-scores across different problems with different class ratios is problematic.\n",
    "\n",
    "**Illustrative example:** <br/>\n",
    "\n",
    "Let's assume that we trained a classification model and the resulting confusion matrix looks as follows:\n",
    "\n",
    "|              | Predicted Positive | Predicted Negative |\n",
    "|:------------:|:------------------:|:------------------:|\n",
    "| **Positive** |   900              |     50             |\n",
    "| **Negative** |   150              |    800             |\n",
    "\n",
    "As a result, the precision $P= \\frac{900}{900+150}=0.857$ and the recall $R=\\frac{900}{900+50}=0.9$ <br/>\n",
    "The resulting F1-score is $(2 \\times 0.857 \\times 0.9) / (0.857+0.9) = 0.878$\n",
    "\n",
    "Let's now assume that we double the number of positives by adding each positive sample twice. Consequently, the resulting confusion matrix looks as follows:\n",
    "\n",
    "|              | Predicted Positive | Predicted Negative |\n",
    "|:------------:|:------------------:|:------------------:|\n",
    "| **Positive** |   1800             |     100             |\n",
    "| **Negative** |   150              |    800             |\n",
    "\n",
    "The precision $P= \\frac{1800}{1800+150}=0.923$ and the recall $R=\\frac{1800}{1800+200}=0.9$ <br/>\n",
    "The resulting F1-score is $(2 \\times 0.923 \\times 0.9) / (0.923+0.9) = 0.911$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a18d52",
   "metadata": {},
   "source": [
    "### Quiz:\n",
    "\n",
    "We are given the following models:\n",
    "    \n",
    "1. A self-driving car model for detecting a pedestrian based on the image from the car’s\n",
    "camera\n",
    "2. A medical model for diagnosing a deadly illness based on the patient’s symptoms\n",
    "3. A recommendation system for movies based on the user's previous movies watched\n",
    "4. A voice assistant that determines whether the user needs assistance given the voice\n",
    "command\n",
    "5. A spam-detection model that determines whether an email is spam based on the words in\n",
    "the email\n",
    "\n",
    "What value of $\\beta$ would you use to evaluate each of the models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a38fb3",
   "metadata": {},
   "source": [
    "## Receiver operating characteristic (ROC) curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74f7abd",
   "metadata": {},
   "source": [
    "The **receiver operating characteristic (ROC)** curve is another common tool used with\n",
    "binary classifiers. \n",
    "\n",
    "The ROC curve plots the true positive rate (**TPR**; a.k.a. **recall/sensitivity**) against the false positive rate (FPR). The FPR is the ratio of negative instances that are incorrectly classified as positive. It is equal to one minus the true negative rate (TNR), which is the ratio of negative instances that are correctly classified as negative. The **TNR is also called specificity**. \n",
    "\n",
    "**The ROC curve shows the TPR (sensitivity) on the y-axis and FPR (1 - specificity) on the x-axis**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91f1b42",
   "metadata": {},
   "source": [
    "<img src=\"img/example_roc_curve.png\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a00c5d",
   "metadata": {},
   "source": [
    "### Sensitivity (a.k.a. True positive rate)\n",
    "\n",
    "The ratio between the number of true positives and the total number of positives.\n",
    "\n",
    "$Sensitivity = \\frac{TP}{P} = \\frac{TP}{TP+FN}$\n",
    "\n",
    "**As can be seen, sensitivity is the same as recall!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0aa4f37",
   "metadata": {},
   "source": [
    "### Specificity (a.k.a. True negative rate)\n",
    "\n",
    "The ratio between the number of true negatives and the total number of negatives.\n",
    "\n",
    "$Specificity = \\frac{TN}{N} = \\frac{TN}{TN + FP}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf7329b",
   "metadata": {},
   "source": [
    "### How do I plot the ROC curve?\n",
    "\n",
    "In order to be able to plot the ROC curve, the only assumption we need to make about our model is that it returns the prediction as a continuous value, namely, as a probability. This is true about models such as logistic\n",
    "classifiers, where the prediction is not a class, such as positive/negative, but a value between 0 and\n",
    "1, such as 0.7. What we normally do with this value is pick a threshold, such as 0.5, and classify\n",
    "every point that receives a prediction higher than or equal to the threshold as positive and every\n",
    "other point as negative. However, this threshold can be any value - it need not be 0.5. Our procedure consists in varying this threshold from 0 all the way to 1 and recording the sensitivity and specificity of the model at each threshold value.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c155bf2",
   "metadata": {},
   "source": [
    "**Example:**\n",
    "    \n",
    "Let's look at an example shown in the figure. We calculate the sensitivity and specificity for three different thresholds: 0.2, 0.5, and 0.8.\n",
    "\n",
    "On the left, we have a model with a low threshold; in the middle, we have one with a medium threshold; and on the right, we have one with a high threshold. For each of the models, there are five positive and five negative points. Each model is represented by the vertical line. The model predicts that the points to the right of the line are positive and those to the left are negative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f7c177",
   "metadata": {},
   "source": [
    "<img src=\"img/roc_curve_example.png\" width=\"600px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84694774",
   "metadata": {},
   "source": [
    "As can be seen, if we increase the threshold (i.e., as we move the vertical line from left to right), the sensitivity goes down and the specificity goes up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49eda83",
   "metadata": {},
   "source": [
    "As we have ten different samples, we can calculate the sensitivity and specificity for ten different thresholds. The sensitivity and specificity obtained for each of the ten thresholds are shown in the table below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d5f3bd",
   "metadata": {},
   "source": [
    "<img src=\"img/roc_curve_list.png\" width=\"600px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3aaf452",
   "metadata": {},
   "source": [
    "Once we obtained this list, we can plot the sensitivity-specificity curve, which is essentially the ROC curve with the x-axis flipped."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0c2e19",
   "metadata": {},
   "source": [
    "<img src=\"img/roc_curve.png\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99e9f02e",
   "metadata": {},
   "source": [
    "### AUC (Area under Curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfadffe",
   "metadata": {},
   "source": [
    "A way to the overall performance of a classifier without the need to choose a threshold is to measure the **area under the curve (AUC)**. <br/> A perfect classifier will have a ROC AUC equal to 1, whereas a purely random classifier will have a ROC AUC equal to 0.5.\n",
    "\n",
    "However, note that the ROC AUC (and thus the ROC curve), tends to be less \"sensitive\" to class imbalances. Therefore, if we want to prioritize minimizing false positives (even if that results in a significant increase in false negatives). AUC ROC isn't a useful metric for this type of optimization. Instead, we could use a precision-recall curve. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a47975",
   "metadata": {},
   "source": [
    "**Hint:** If the measured AUC is less than 0.5, this usually indicates that you wrongly assigned the class labels of your prediction. Note that if the reported AUC ROC = 0.2, an AUC = 1 - 0.2 = 0.8 can be obtained by flipping the class label of the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae74200b",
   "metadata": {},
   "source": [
    "<img src=\"img/roc_curve_comparison.png\" width=\"400px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db698ed8",
   "metadata": {},
   "source": [
    "### How do I determine the threshold?\n",
    "\n",
    "Every model requires some amount of sensitivity and specificity based on the problem we are solving. Having access to the ROC curve, we can find the optimal model for our needs by selecting a point on the ROC curve and determining the corresponding threshold. \n",
    "\n",
    "- The lower the threshold, the lower the TPR (sensitivity) and the higher the TNR (specificity).\n",
    "- The higher the threshold, the higher the TPR (sensitivity) and the lower the TNR (specificity)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635ed604",
   "metadata": {},
   "source": [
    "## Metric Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92275be",
   "metadata": {},
   "source": [
    "$Accuracy = \\frac{\\text{# Correct predictions}}{\\text{# Num Predictions}} = \\frac{TP + FP}{TP + FP + FN + FP}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc4efc7",
   "metadata": {},
   "source": [
    "$F_\\beta = \\frac{(1+\\beta^2)PR}{\\beta^2 P+R}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7afb73",
   "metadata": {},
   "source": [
    "<img src=\"img/summary_metrics.png\" width=\"600px\"/>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
