{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b42abdca",
   "metadata": {},
   "source": [
    "# Performance evaluation metrics in Scikit-Learn\n",
    "\n",
    "\n",
    "In this notebook, we will look at how to evaluate the performance of a model using Scikit-Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64ae4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import brentq\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739c4904",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import fbeta_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.metrics import PrecisionRecallDisplay\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afde2e0",
   "metadata": {},
   "source": [
    "## Load the MNIST dataset\n",
    "\n",
    "The MNIST dateaset is a large dataset of handwritten digits that were normalized to fit into a 28x28 pixel bounding box. The database is also widely used for training and testing in the field of machine learning. The MNIST database contains 60000 training images and 10000 testing images.\n",
    "\n",
    "To better see the effect of class imbalances, we turn classifying MNIST into a binary classification problem by using digit \"5\" as the positive label and the remaining digits as the negative label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9de786",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = datasets.fetch_openml('mnist_784', parser='pandas')\n",
    "\n",
    "x = db.data.values\n",
    "\n",
    "# Change class labels to 1 (digit 5) and 0 (otherwise)\n",
    "y = np.where(db.target == '5', 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f834ba05",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_ratio = (y == 0).sum() / (y == 1).sum()\n",
    "\n",
    "print(f'There are {class_ratio:.1f}x more samples with label != 5 than label == 5.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f974a31",
   "metadata": {},
   "source": [
    "As can be seen, our binary classification problem is heavily imbalanced. We have 10 times more positive samples than negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50288c7",
   "metadata": {},
   "source": [
    "## Use STRATIFIED random splitting to split the dataset into a training and test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4dba97",
   "metadata": {},
   "source": [
    "**Stratified** random sampling ensures that the returned sets have the same proportion of instances for each class as the original dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a5ec86",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc71f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Num training samples:', x_train.shape[0])\n",
    "print('Num testing samples:', x_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12f77d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_ratio = (y_test == 0).sum() / (y_test == 1).sum()\n",
    "print(f'There are {class_ratio:.1f}x more samples in the test set with label != 5 than label == 5.')\n",
    "\n",
    "class_ratio = (y_train == 0).sum() / (y_train == 1).sum()\n",
    "print(f'There are {class_ratio:.1f}x more samples in the training set with label != 5 than label == 5.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d1925b",
   "metadata": {},
   "source": [
    "## Train a logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b279f209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize features by removing the mean and scaling to unit variance.\n",
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(x_train)\n",
    "\n",
    "x_train = scaler.transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5280a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a logistic regression model\n",
    "model = LogisticRegression(max_iter=500)\n",
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d25871ee",
   "metadata": {},
   "source": [
    "## Performance evaluation with different metrics\n",
    "\n",
    "In the following, we will demonstrate how to evaluate the performance of the model with different metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19aa00b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label = model.predict(x_test)\n",
    "pred_proba = model.predict_proba(x_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32ace19f",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37444aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Evaluate the accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643e9737",
   "metadata": {},
   "source": [
    "Less surprising, the accuracy is relatively high. Yet, it's unclear how much of the high accuracy is caused by the class imbalance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522677b5",
   "metadata": {},
   "source": [
    "### Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5047f30b",
   "metadata": {},
   "source": [
    "Scikit's `confusion_matrix()` can compute the confusion matrix for a given set of class predictions and target classes. The method can also handle problems where more than two classes are available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c64943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Obtain the values of the confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2a494d",
   "metadata": {},
   "source": [
    "In order to nicely visualize the confusion matrix, we can use **Seaborn**. Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.\n",
    "\n",
    "It's a really nice library and definitely worth looking at!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16a0053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use matplotlib to plot the confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ba039f",
   "metadata": {},
   "source": [
    "Alternatively, we can use Scikit's built-in functionalities for plotting confusion matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8fdad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use ConfusionMatrixDisplay from Scikit to plot the confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fecf4c67",
   "metadata": {},
   "source": [
    "### Precision and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32870d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate precision and recall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d16b38f",
   "metadata": {},
   "source": [
    "In contrast to the model's accuracy, precision and recall are relatively low. Among the samples predicted to be positive, only 89% were positive. Only 81% of the positive samples were correctly classified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0acc7d2",
   "metadata": {},
   "source": [
    "### F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de09f035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate the F1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b74073",
   "metadata": {},
   "source": [
    "### F-beta score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901a229e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate the f-beta score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef244df3",
   "metadata": {},
   "source": [
    "### Precision-Recall Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca40bdf",
   "metadata": {},
   "source": [
    "The Precision-Recall curve shows precision and recall for different threshold levels.\n",
    "\n",
    "Note that \"AP\" denotes the average precision.\n",
    "\n",
    "AP summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold, with the increase in recall from the previous threshold used as the weight:\n",
    "\n",
    "$AP = \\sum_n (R_n - R_{n-1}) P_n$\n",
    "\n",
    "where $P_n$ and $R_n$ are the precision and recall at the $n$-th threshold. This implementation is not interpolated and is different from computing the area under the precision-recall curve with the trapezoidal rule, which uses linear interpolation and can be too optimistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74cbc7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the precision-recall curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a97c22d",
   "metadata": {},
   "source": [
    "### Sensitivity and Specificity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351bf41d",
   "metadata": {},
   "source": [
    "Unfortunately, Scikit does not provide dedicated methods to calculate the sensitivity or specificity of a model. However, we can calculate the metric ourselves from the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acbd4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cf_matrix = confusion_matrix(y_test, pred_label)\n",
    "\n",
    "tn, fp, fn, tp = cf_matrix.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98649279",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_sensitivity = tp / (tp + fn)\n",
    "score_specificity = tn / (tn + fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38db2251",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Sensitivity (TPR):', score_sensitivity)\n",
    "print('Specificity (TNR):', score_specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e495ec",
   "metadata": {},
   "source": [
    "As can be seen, almost all negative samples (99%) are predicted correctly. However, only 81% of the positive samples are predicted correctly. Clearly, our classifier performs worse on positive samples than on negative ones."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1351d1c3",
   "metadata": {},
   "source": [
    "### ROC Curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcac5d5",
   "metadata": {},
   "source": [
    "Scikit's `RocCurveDisplay` can be used to generate the ROC curve. The `roc_auc_score()` function directly computes the AUC-ROC for some given prediction scores / targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6617f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Plot the ROC curve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97097479",
   "metadata": {},
   "source": [
    "Note that the AUC ROC achieved by our model is 97.7\\%. This isn't great, but it is still high compared to the APR (90.4\\%)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f947708b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternatively, we can .from_estimator() method to obtain the plot in just one line\n",
    "display = RocCurveDisplay.from_estimator(model, x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3f5655",
   "metadata": {},
   "source": [
    "### EER (Equal Error Rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656fa9c0",
   "metadata": {},
   "source": [
    "The rate at which the false positive rate and false negative rate are equal. The value of the EER can be easily obtained from the ROC curve. The EER is a quick way to compare the accuracy of devices with different ROC curves. In general, the device with the lowest EER is the most accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e83cfbcb",
   "metadata": {},
   "source": [
    "Unfortunately, Scikit does not provide a dedicated method to calculate the EER. So we have to manually calculate the EER from the FPRs and TPRs returned by `roc_curve()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c678125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate the equal error rate from roc_curve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a38333b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display = RocCurveDisplay.from_estimator(model, x_test, y_test)\n",
    "plt.plot([0, 1], [1, 0], 'k--')\n",
    "plt.scatter(eer, 1.-eer, c='r', label='EER')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135ab99c",
   "metadata": {},
   "source": [
    "As can be seen, if we want to choose the threshold such that the FPR = FNR, the threshold is 0.0827. <br/>\n",
    "This threshold is totally different from the default threshold (0.5)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a8a691",
   "metadata": {},
   "source": [
    "Let's see what happens to precision and recall if we choose this threshold for choosing the final class label ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24e52ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Calculate precision and recall for a given threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4decac",
   "metadata": {},
   "source": [
    "Choosing a different threshold has a tremendous effect on precision and recall. Only 61\\% of the samples classified as positive were positive. However, the recall increased to 94\\%, which means that 94\\% of the positive samples were correctly classified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417f1d48",
   "metadata": {},
   "source": [
    "**This illustrates the importance of choosing the \"right\" metric for choosing a classifier.** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c795fdc3",
   "metadata": {},
   "source": [
    "### Precision/Recall Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9e96d7",
   "metadata": {},
   "source": [
    "We classify samples with *pred_proba* greater than the threshold as positive. If we increase the threshold, fewer samples will be classified as positives, and therefore the recall (a.k.a. tpr) has to decrease. On the other hand, precision has to increase since we classify fewer samples as positives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1868f89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend()\n",
    "    plt.ylim([0, 1])\n",
    "\n",
    "\n",
    "precisions, recalls, thresholds = precision_recall_curve(y_test, pred_proba)\n",
    "\n",
    "plot_precision_recall_vs_threshold(precisions, recalls, thresholds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3064887e",
   "metadata": {},
   "source": [
    "### Sensitivity/Specificity Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a5e5dc",
   "metadata": {},
   "source": [
    "We classify samples with *pred_proba* greater than the threshold as positive. Consequently, if we increase the threshold, fewer samples will be classified as positives, and therefore the sensitivity (a.k.a. tpr) has to decrease. On the other hand, specificity (a.k.a. TNR) has to increase since more samples are classified as negatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893dc18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sensitivity_specificity_vs_threshold(sensitivity, specificity, thresholds):\n",
    "    plt.plot(thresholds, sensitivity, \"b--\", label='Sensitivity (a.k.a. TPR)')\n",
    "    plt.plot(thresholds, specificity, \"g-\", label='Specificity (a.k.a. TNR)')\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.legend()\n",
    "    plt.ylim([0, 1])\n",
    "\n",
    "\n",
    "fprs, tprs, thresholds = roc_curve(y_test, pred_proba)\n",
    "\n",
    "specificity = 1 - fprs\n",
    "sensitivity = tprs\n",
    "\n",
    "plot_sensitivity_specificity_vs_threshold(sensitivity, specificity, thresholds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac8e3fd",
   "metadata": {},
   "source": [
    "## Using different performance metric with cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d47ba21",
   "metadata": {},
   "source": [
    "In our previous examples, we used cross-validation to find the best parameters for our model. This raises the question of how we can tell Scikit's `cross_val()` or `cross_val_score()` method to use a different evaluation metric. \n",
    "\n",
    "A different evaluation metric can be configured via the `scoring` attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e119041",
   "metadata": {},
   "source": [
    "**Hint:** The mean and standard deviation should be estimated for each training fold individually. To achieve this, Scikit provides the `Pipeline` class. A pipeline defines a chain of transformations that are applied to your data set sequentially, where the last step in the chain is your machine learning model. We use `make_pipeline()` to construct a model training pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c6bff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make_pipelines ensures that StandardScaler() fit on each training fold individually\n",
    "pipeline = make_pipeline(preprocessing.StandardScaler(), \n",
    "                         LogisticRegression(max_iter=500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9942bca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 4-fold cross validation and use the f1-score to measure the model performance\n",
    "cross_val_score(pipeline, x, y, cv=4, scoring=\"f1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fad2b89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 4-fold cross validation and use the accuracy to measure the model performance\n",
    "cross_val_score(pipeline, x, y, cv=4, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231a47d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use 4-fold cross validation and use the precision to measure the model performance\n",
    "cross_val_score(pipeline, x, y, cv=4, scoring=\"precision\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
