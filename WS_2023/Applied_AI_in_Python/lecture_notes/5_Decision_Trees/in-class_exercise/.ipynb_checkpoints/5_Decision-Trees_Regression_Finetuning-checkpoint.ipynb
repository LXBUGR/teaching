{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d312d792",
   "metadata": {},
   "source": [
    "# Decision Trees for Regression\n",
    "\n",
    "In this example, we will learn how to train a decision tree with **scikit-learn**. <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e135ec84",
   "metadata": {},
   "source": [
    "#### Dataset: California Housing\n",
    "\n",
    "As an example dataset we will use the California Housing dataset. The data contains information from the 1990 California census.  A description of this dataset can be found here: https://www.kaggle.com/datasets/camnugent/california-housing-prices\n",
    "\n",
    "Fortunately, the dataset is already provided in scikit-learn. So, there is no need to fetch the data from Kaggle manually. See: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a127b986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules which are relevant for this project\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "284dae70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To ensure reproducibility\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70015490",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11f82cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fetch_california_housing(as_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c083760",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df = dataset['data']\n",
    "target_df = dataset['target']\n",
    "\n",
    "housing_df['HousePrice'] = target_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7dbf9f",
   "metadata": {},
   "source": [
    "## Prepare a training, validation and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3279dcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(housing_df, test_size=0.2)\n",
    "\n",
    "# Prepare training features \n",
    "x_train = train_df.loc[:, train_df.columns != 'HousePrice'].values\n",
    "y_train = train_df['HousePrice'].values\n",
    "\n",
    "# Prepare testing features \n",
    "x_test = test_df.loc[:, test_df.columns != 'HousePrice'].values\n",
    "y_test = test_df['HousePrice'].values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc6ed58",
   "metadata": {},
   "source": [
    "## Searching the optimal hyperparameters using grid search\n",
    "\n",
    "We have now learned that decision trees cannot only be regularized by their maximum depth but also by other hyperparameters, such as the minimum number of samples required in a leaf node. Ideally, in order to find the optimal choice of hyperparameters, many different combinations of hyperparameters have to be tested. While in the case of two hyperparameters, this is still easy to achieve with a nested for-loop, this process becomes tedious if we have to explore more hyperparameters.\n",
    "\n",
    "To simplify the process of finding the right hyperparamter, scikit-learn provides the class `GridSearchCV`.`GridSearchCV` does an exhaustive search over specified parameter values for an estimator (model). All you need to do is tell it which hyperparameters you want it to experiment with, and what values to try out, and it will evaluate all the possible combinations of hyperparameter values using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "929588d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the hyperparameters that should be tested.\n",
    "# We want to test different parameters for max_depth (values from 5 to 30) and\n",
    "# min_samples_leaf (values from 1 to 10)\n",
    "param_grid = [\n",
    "    {\n",
    "        'max_depth': range(5, 30), \n",
    "        'min_samples_leaf': range(1, 10)\n",
    "    }\n",
    "]\n",
    "\n",
    "# TODO: Use grid search to determine the optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8892d718",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Best hyperparameters:', grid_search.best_params_)\n",
    "print('Best score:', grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09988766",
   "metadata": {},
   "source": [
    "## Evaluate the model performance on the testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e753aed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeRegressor(**grid_search.best_params_)\n",
    "\n",
    "# Re-Train the model on the entire training set with the best hyperparameters \n",
    "# found by grid search.\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "rmse_test = mean_squared_error(y_test, y_pred_test, squared=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f04c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'RMSE on the training set: ${rmse_test*100000}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1be588",
   "metadata": {},
   "source": [
    "### What are the most important features?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd185c48",
   "metadata": {},
   "source": [
    "If we look at a decision tree, the most important features are likely to appear closer to the root of the tree, while less important features often appear closer to the leaves (or not at all). <br/>\n",
    "This is because at each level, the decision tree regressor picks the feature that results in the largest decrease in the error criterion (e.g., MSE). We can quantify the importance of a feature by computing the (normalized) total reduction of the criterion brought by that feature.\n",
    "\n",
    "See: https://github.com/scikit-learn/scikit-learn/blob/5444030d064d89bd4001efdb367f2c9685847f9a/sklearn/tree/_tree.pyx#L1266C39-L1266C39"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c94e249e",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8a4a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the names of the features used for training\n",
    "feature_names = train_df.loc[:, train_df.columns != 'HousePrice'].columns\n",
    "\n",
    "# Sort features in ascending order\n",
    "sorted_indices = feature_importances.argsort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cce51b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.barh(feature_names[sorted_indices], feature_importances[sorted_indices])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
