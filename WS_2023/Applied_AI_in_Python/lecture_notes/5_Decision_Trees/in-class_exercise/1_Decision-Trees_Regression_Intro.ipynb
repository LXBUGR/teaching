{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d312d792",
   "metadata": {},
   "source": [
    "# Decision Trees for Regression\n",
    "\n",
    "In this example, we will learn how to train a decision tree with **scikit-learn**. <br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e135ec84",
   "metadata": {},
   "source": [
    "#### Dataset: California Housing\n",
    "\n",
    "As an example dataset we will use the California Housing dataset. The data contains information from the 1990 California census.  A description of this dataset can be found here: https://www.kaggle.com/datasets/camnugent/california-housing-prices\n",
    "\n",
    "Fortunately, the dataset is already provided in scikit-learn. So, there is no need to fetch the data from Kaggle manually. See: https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_california_housing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a127b986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules which are relevant for this project\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284dae70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To ensure reproducibility\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70015490",
   "metadata": {},
   "source": [
    "## Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f82cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fetch_california_housing(as_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c083760",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df = dataset['data']\n",
    "target_df = dataset['target']\n",
    "\n",
    "housing_df['HousePrice'] = target_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a7dbf9f",
   "metadata": {},
   "source": [
    "## Prepare a training and testing set\n",
    "\n",
    "Like we did in our linear regression example, we split the datasets into two parts. <br/>\n",
    "80% of the data is used for training, and 20% is used for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3279dcef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(housing_df, test_size=0.2)\n",
    "\n",
    "# Prepare training features \n",
    "x_train = train_df.loc[:, train_df.columns != 'HousePrice'].values\n",
    "y_train = train_df['HousePrice'].values\n",
    "\n",
    "# Prepare testing features \n",
    "x_test = test_df.loc[:, test_df.columns != 'HousePrice'].values\n",
    "y_test = test_df['HousePrice'].values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc6ed58",
   "metadata": {},
   "source": [
    "## Training the first decision tree regressor\n",
    "\n",
    "We now train our first decision tree, more precisely, a decision tree regressor also referred to as `DecisionTreeRegressor` in scikit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929588d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Traing and fit a decision tree regressor\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53a16c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the performance of the regression model\n",
    "y_pred_train = model.predict(x_train)\n",
    "y_pred_test = model.predict(x_test)\n",
    "\n",
    "rmse_test = mean_squared_error(y_test, y_pred_test, squared=False)\n",
    "rmse_train = mean_squared_error(y_train, y_pred_train, squared=False)\n",
    "\n",
    "print('Train RMSE:', rmse_train)\n",
    "print('Test RMSE:', rmse_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee15e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question: What can we say about the result?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00aaf27f",
   "metadata": {},
   "source": [
    "### How tree depth affects overfitting\n",
    "\n",
    "The depth of a decision tree controls its expressiveness. In our case, this means that the deeper a tree becomes, the more precise it can predict the value of a sample. However, the decision tree also becomes more likely to overfit the training data.\n",
    "\n",
    "To better understand how this looks like in practice, we can plot the RMSE for the training and testing set for different tree depths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f88342",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for depth in range(1, 36):\n",
    "    \n",
    "    model = DecisionTreeRegressor(max_depth=depth)\n",
    "    \n",
    "    model.fit(x_train, y_train)\n",
    "    \n",
    "    rmse_test = mean_squared_error(y_test, model.predict(x_test), squared=False)\n",
    "    rmse_train = mean_squared_error(y_train, model.predict(x_train), squared=False)\n",
    "\n",
    "    train_scores.append((depth, rmse_train))\n",
    "    test_scores.append((depth, rmse_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428f7728",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = zip(*train_scores)\n",
    "plt.plot(x, y, label='Training Error (RMSE)')\n",
    "\n",
    "x, y = zip(*test_scores)\n",
    "plt.plot(x, y, label='Generalization Error (RMSE)')\n",
    "\n",
    "plt.scatter(x[np.argmin(y)], np.min(y), s=30, c='g', label=f'Lowest Generalization Error \\nat depth={x[np.argmin(y)]}')\n",
    "\n",
    "plt.xlabel('Max Tree Depth')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87989c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question: How can we determine the optimal tree depth?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
