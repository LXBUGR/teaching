{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d312d792",
   "metadata": {},
   "source": [
    "# Decision Trees for Classification\n",
    "\n",
    "In this notebook, we learn how decision trees can solve classification problems and how they are constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c507836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import export_graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69db2703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To install graphviz run: \n",
    "# ==> conda install python-graphviz\n",
    "from graphviz import Source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fe13ba",
   "metadata": {},
   "source": [
    "## Decision Tree Classifier\n",
    "\n",
    "So far, we have only used decision trees in the context of regression tasks. We will now learn how to apply decision trees to classification tasks. Classification is the process of finding a model (e.g., a decision tree) that is capable of dividing a dataset into different classes.\n",
    "\n",
    "Scikit-Learn provides the class `DecisionTreeClassifier` for building decision trees that can solve classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85e23eb",
   "metadata": {},
   "source": [
    "### Load sample dataset\n",
    "\n",
    "In order to study how a decision tree classifier works, we need a sample dataset. We use the Iris Flower (dt. \"Schwertlilie\") dataset for this purpose.\n",
    "\n",
    "The dataset consists of 50 samples from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals, in centimeters.\n",
    "\n",
    "Our goal is to predict the species based on the sample features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c105e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_db = load_iris(as_frame=True)\n",
    "iris_df = iris_db.frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87c16a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Inspect the features of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e997468a",
   "metadata": {},
   "source": [
    "**According to the description and the `target_names` attribute, the class labels represent the following species:**\n",
    "\n",
    "- Setosa\n",
    "- Versicolour\n",
    "- Virginica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e1077b",
   "metadata": {},
   "source": [
    "### Train a simple decision tree classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556e90be",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = iris_df[['petal length (cm)', 'petal width (cm)']].values\n",
    "y = iris_df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2dbc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train a decision tree classifier on <x, y>\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ac3a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question: Check the documentation. \n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "# What error function does the decision tree classifier optimize?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b17972bb",
   "metadata": {},
   "source": [
    "### Visualize the decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e29fdae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "export_graphviz(\n",
    "    model,\n",
    "    out_file='iris_tree.dot',\n",
    "    feature_names=[\"petal length (cm)\", \"petal width (cm)\"],\n",
    "    class_names=iris_db.target_names,\n",
    "    rounded=True,\n",
    "    filled=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a409f652",
   "metadata": {},
   "outputs": [],
   "source": [
    "Source.from_file('iris_tree.dot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07cecef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question: \n",
    "# How does the decision tree look like?\n",
    "# What are the thresholds?\n",
    "# How many items per node?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad0f38c",
   "metadata": {},
   "source": [
    "### Visualization of the prediction boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97ddb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "custom_cmap = ListedColormap(['#fafab0', '#9898ff', '#a0faa0'])\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "# Obtain testing samples by taking point on a regular-spaced grid \n",
    "lengths, widths = np.meshgrid(np.linspace(0, 7.2, 100), \n",
    "                              np.linspace(0, 3, 100))\n",
    "\n",
    "# ravel() returns a flatten array. Unlike flatten(), it does not return a copy \n",
    "# of the original array\n",
    "x_all = np.stack((lengths.ravel(), widths.ravel()), axis=1)\n",
    "\n",
    "# Predict the class of each sample point\n",
    "y_pred = model.predict(x_all).reshape(lengths.shape)\n",
    "\n",
    "# Create a contour plot using the regular-spaced grid (+ the predict class)\n",
    "plt.contourf(lengths, widths, y_pred, alpha=0.3, cmap=custom_cmap)\n",
    "\n",
    "# Visualize the training samples (+ their class value)\n",
    "for idx, (name, style) in enumerate(zip(iris_db.target_names, (\"yo\", \"bs\", \"g^\"))):\n",
    "    plt.plot(x[:, 0][y == idx], x[:, 1][y == idx],\n",
    "             style, label=f\"{name}\")\n",
    "\n",
    "plt.xlabel(\"Petal length (cm)\")\n",
    "plt.ylabel(\"Petal width (cm)\")\n",
    "plt.axis([0, 7.2, 0, 3])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea1548a",
   "metadata": {},
   "source": [
    "### How to construct a decision tree for classification?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52015e5",
   "metadata": {},
   "source": [
    "We have already introduced the **Classification And Regression Tree (CART)** algorithm when we learned how to construct a decision tree for regression. The construction of decision trees for a classification problem works in an analogous way. In other words, we also aim to split the dataset into two subsets using a single feature $k$ and a threshold $t_k$. These features should be chosen such that the weighted cost $C(k, t_k)$ is minimal.\n",
    "\n",
    "$C(k, t_k) = \\frac{n_{left}}{n} C_{left} + \\frac{n_{right}}{n} C_{right}$\n",
    "\n",
    "where $C_{left}$ and $C_{right}$ is some cost function computed on the left and right subset, $n_{left}$ and $n_{right}$ denotes the number of samples contained in the left and right subset, and $n$ denotes the total number of samples.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d36056",
   "metadata": {},
   "source": [
    "However, since we are dealing with a classification problem (which implies that our predicted value is categorical), we cannot use the MSE or MAE as a cost function. Instead, we need a metric that is able to deal with categorical values such as the **Gini Index** or **Entropy**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5e3051",
   "metadata": {},
   "source": [
    "#### Gini impurity index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da383e08",
   "metadata": {},
   "source": [
    "The **Gini impurity index is a measure of diversity** in a dataset. In other words, if we have a set in which all the elements are similar, this set has a low Gini index, and if all the elements are different, it has a large Gini index. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3cf328",
   "metadata": {},
   "source": [
    "**Definition:**\n",
    "\n",
    "\n",
    "In a set with $m$ elements and $n$ classes, with $a_i$ elements belonging to the $i$-th class, the Gini impurity index is\n",
    "\n",
    "$Gini = 1 - p_1^2 - p_2^2 - ... - p_n^2$ \n",
    "\n",
    "where $p_i=\\frac{a_i}{m}$. \n",
    "\n",
    "This can be interpreted as the probability that if we pick two random elements out of the set, they belong to different classes.\n",
    "\n",
    "**Note:** The Gini impurity index should not be confused with the Gini coefficient. The Gini coefficient is used in statistics to calculate the income or wealth inequality in countries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3f5541",
   "metadata": {},
   "source": [
    "**The following code illustrates how to compute the Gini impurity index:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbb34e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini_impurity(y):\n",
    "    \n",
    "    pass\n",
    "\n",
    "    # TODO: Calculate the gini impurity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6571a50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_weighted_gini = np.inf\n",
    "split_threshold = -1\n",
    "split_feature_idx = -1\n",
    "\n",
    "# Since each sample is described by two input features, we need to look for the\n",
    "# best possible split in each dimension\n",
    "for feature_idx in range(0, 2):\n",
    "    \n",
    "    x_feat = x[:, feature_idx]\n",
    "    \n",
    "    sort_idx = np.argsort(x_feat, axis=0)\n",
    "    x_sorted = np.take_along_axis(x_feat, sort_idx, axis=0)\n",
    "    y_sorted = np.take_along_axis(y, sort_idx, axis=0)\n",
    "    \n",
    "    for i in range(1, y.shape[0]):\n",
    "        \n",
    "        sort_idx = np.argsort(x, axis=0)\n",
    "\n",
    "        # Get samples in left and right set\n",
    "        y_left = y_sorted[:i]\n",
    "        y_right = y_sorted[i:]\n",
    "\n",
    "        # Number of samples in each set\n",
    "        num_left = len(y_left)\n",
    "        num_right = len(y_right)\n",
    "        num_total = num_left + num_right\n",
    "\n",
    "        # Compute GINI impurity for each set\n",
    "        gini_left = gini_impurity(y_left)\n",
    "        gini_right = gini_impurity(y_right)\n",
    "\n",
    "        # Compute the weight sum of both gini impurities\n",
    "        weighted_gini = num_left / num_total * gini_left + num_right / num_total * gini_right\n",
    "\n",
    "        if weighted_gini < min_weighted_gini:\n",
    "            min_weighted_gini = weighted_gini\n",
    "            split_threshold = (x_sorted[i-1] + x_sorted[i]) / 2\n",
    "            split_feature_idx = feature_idx\n",
    "\n",
    "# Obtain the samples in the left and right child node\n",
    "y_left = y[x[:, split_feature_idx] <= split_threshold]\n",
    "y_right = y[x[:, split_feature_idx] > split_threshold]\n",
    "\n",
    "print(f'Threshold: {split_threshold:.3f}')\n",
    "print(f'Split Feature Idx: {split_feature_idx}')\n",
    "print(f'Gini Impurity Root: {gini_impurity(y)}')\n",
    "print(f'Gini Impurity Left: {gini_impurity(y_left)}')\n",
    "print(f'Gini Impurity Right: {gini_impurity(y_right)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bf47b5",
   "metadata": {},
   "source": [
    "#### Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ffce8e",
   "metadata": {},
   "source": [
    "Another measure of homogeneity in a set is **entropy**. It is is based on the physical concept of entropy and is highly important in probability and information theory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9faea398",
   "metadata": {},
   "source": [
    "**Definition:**\n",
    "    \n",
    "    \n",
    "In a set with $m$ elements and $n$ classes, with $a_i$ elements belonging to the $i$-th class, the entropy is\n",
    "\n",
    "$Entropy = -p_1 log_2(p_1) - p_2 log_2(p_2) – ... – p_n log_2(p_n)$\n",
    "\n",
    "where $p_i = \\frac{a_i}{m}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692de888",
   "metadata": {},
   "source": [
    "#### Entropy vs. Gini impurity\n",
    "\n",
    "So should you use Gini impurity or entropy? The truth is, most of the time it does not make a big difference: they lead to similar trees. Gini impurity is slightly faster to compute, so it is a good default. However, when they differ, Gini impurity tends to isolate the most frequent class in its own branch of the tree, while entropy tends to produce slightly more balanced trees. See also https://sebastianraschka.com/faq/docs/decision-tree-binary.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b97373",
   "metadata": {},
   "source": [
    "### What if we have categorical features?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e76c0ce",
   "metadata": {},
   "source": [
    "So far, in all our decision tree examples, we have only worked with continuous input features. This raises the question of whether we can also process categorical input features. Yes, this is indeed possible, and we can even process non-binary categorical input features if we apply a simple trick.\n",
    "\n",
    "**Binary categorical features:** Let's first assume we are given a dataset where each sample has a feature saying whether it's a cat or a dog. Since it's a binary feature, we assign each label a numeric class label that is either 0 or 1 depending on the label. When training a decision tree, the numeric labels are simply considered as continuous values. Consequently, the decision threshold that splits the samples into two subsets is 0.5. \n",
    "\n",
    "**Non-Binary categorical features:** Let's now assume we are given a dataset where each sample has a feature saying whether it's a cat, dog, or Bird. Obviously, this feature is not binary as there are three possible labels. However, we can convert the feature into several binary features using so-called **one-hot encoding**. In our example, one-hot encoding the feature would result in three distinct binary features encoding the \"questions\":\n",
    "- Is the animal a dog?\n",
    "- Is the animal a cat?\n",
    "- Is the animal a bird?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1489479c",
   "metadata": {},
   "source": [
    "Note that Scikit's provides the class `OneHotEncoder` which is able to produce a one-hot encoding of a given feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2bece293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Demonstrate how the OneHotEncoder works"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc66bd8",
   "metadata": {},
   "source": [
    "## Instability\n",
    "\n",
    "We have seen that decision trees are simple to understand and interpret, easy to use, versatile, and powerful.\n",
    "However they do have a few limitations. First, as you may have noticed, Decision Trees love orthogonal decision boundaries (all splits are perpendicular to an axis), which makes them sensitive to training set rotation. For example, consider a simple linearly separable dataset: on the left, a Decision Tree can split it easily, while\n",
    "on the right, after the dataset is rotated by 45°, the decision boundary looks unnecessarily convoluted. Although both Decision Trees fit the training set perfectly, it is very likely that the model on the right will not generalize well.\n",
    "\n",
    "More generally, the main issue with Decision Trees is that they are very sensitive to small variations in the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47781ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a linearily separable dataset\n",
    "np.random.seed(6)\n",
    "\n",
    "x = np.random.rand(100, 2) - 0.5\n",
    "y = (x[:, 0] > 0).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77af2148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rotate the dataset\n",
    "\n",
    "angle = np.pi / 4  # 45 degrees\n",
    "rotation_matrix = np.array([[np.cos(angle), -np.sin(angle)],\n",
    "                            [np.sin(angle), np.cos(angle)]])\n",
    "x_rot = x.dot(rotation_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b98e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(clf, X, y, axes, cmap):\n",
    "    \n",
    "    x1, x2 = np.meshgrid(np.linspace(axes[0], axes[1], 100),\n",
    "                         np.linspace(axes[2], axes[3], 100))\n",
    "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
    "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
    "    \n",
    "    plt.contourf(x1, x2, y_pred, alpha=0.3, cmap=cmap)\n",
    "    plt.contour(x1, x2, y_pred, cmap=\"Greys\", alpha=0.8)\n",
    "    colors = {\"Wistia\": [\"#78785c\", \"#c47b27\"], \"Pastel1\": [\"red\", \"blue\"]}\n",
    "    markers = (\"o\", \"^\")\n",
    "    \n",
    "    for idx in (0, 1):\n",
    "        plt.plot(X[:, 0][y == idx], X[:, 1][y == idx],\n",
    "                 color=colors[cmap][idx], marker=markers[idx], linestyle=\"none\")\n",
    "    plt.axis(axes)\n",
    "    plt.xlabel(r\"$x_1$\")\n",
    "    plt.ylabel(r\"$x_2$\", rotation=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8271878",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a classifier to separate the two classes\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "rot_model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "model.fit(x, y)\n",
    "rot_model.fit(x_rot, y)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
    "\n",
    "# plt.sca(): Set the current Axes to ax and the current Figure to the parent of ax.\n",
    "plt.sca(axes[0])\n",
    "plot_decision_boundary(model, x, y,\n",
    "                       axes=[-0.7, 0.7, -0.7, 0.7], cmap=\"Pastel1\")\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plot_decision_boundary(rot_model, x_rot, y,\n",
    "                       axes=[-0.7, 0.7, -0.7, 0.7], cmap=\"Pastel1\")\n",
    "plt.ylabel(\"\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
