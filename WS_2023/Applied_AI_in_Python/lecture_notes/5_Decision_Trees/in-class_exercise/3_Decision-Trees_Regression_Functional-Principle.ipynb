{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d312d792",
   "metadata": {},
   "source": [
    "# Decision Trees for Regression\n",
    "\n",
    "In this notebook, we learn how decision trees can solve regression problems and how they are constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c507836",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import export_graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69db2703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To install graphviz run: \n",
    "# ==> conda install python-graphviz\n",
    "from graphviz import Source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fe13ba",
   "metadata": {},
   "source": [
    "## Decision Tree Regressor\n",
    "\n",
    "As we have seen in the California Housing example, decision trees can be applied to regression tasks. In order to learn how decision tree can solve such regression problem, we will take a look at a simple example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f782790b",
   "metadata": {},
   "source": [
    "### Create sample dataset (Quadratic Function)\n",
    "\n",
    "Let's assume we are given the following dataset composed of points sampled from a quadratic function. Our goal is to predict the values of the quadratic function with a decision tree regressor. In other words, we try to \"fit\" the quadratic function with our decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e044b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "num_samples = 200\n",
    "x = (np.random.rand(num_samples, 1) - 0.5)\n",
    "y = x ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6771f43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y, s=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0029dde7",
   "metadata": {},
   "source": [
    "#### Visualizing a decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0744ee50",
   "metadata": {},
   "source": [
    "We now train Scikit's `DecisionTreeRegressor` to predict the `y` value for each sample point `x`. To allow a better visualization of the decision tree, we restrict its maximum depth to 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f2b762",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the Mean Squared error to determine the best split\n",
    "# MSE is also used as the default\n",
    "model = DecisionTreeRegressor(max_depth=2, criterion='squared_error')\n",
    "model.fit(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1c63ac7",
   "metadata": {},
   "source": [
    "Scikit's `export_graphviz()` method allows you to export a decision tree in DOT format. This function generates a GraphViz representation of the decision tree, which is then written into `out_file`. The DOT format is a graph description language developed as part of the Graphviz project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c5c30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use export_graphviz() to create a graphical representation of the decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbff5b82",
   "metadata": {},
   "source": [
    "In order to visualize the graph, we can either use online tools such as https://edotor.net/ or the `graphviz` package to directly inspect the tree in the jupyter notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290062e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Open and show the decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86ee52f",
   "metadata": {},
   "source": [
    "### How do we predict the value for a given x?\n",
    "\n",
    "Given this tree, let's now suppose you want to make a prediction for a new sample with x = 0.31. We traverse\n",
    "the tree starting at the root. We ask whether x<=-3.35, which is false and move to the root's right child node. We ask whether x <= 0.323, which is true. We move to the left child node and now have to stop since we reached a leave node. We return the mean y-value of all samples (which is 0.042) in the leaf node as our estimated value."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57924f0f",
   "metadata": {},
   "source": [
    "### Visualization of the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c2a72e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.linspace(-0.5, 0.5, 1000).reshape(-1, 1)\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "plt.plot(x_test, y_pred, c='r')\n",
    "\n",
    "plt.xlim(-0.5, 0.5)\n",
    "plt.scatter(x, y, s=3)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('max_depth = 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accf98e6",
   "metadata": {},
   "source": [
    "### How do we determine the optimal split at each level?\n",
    "\n",
    "We now understand how to \"parse\" a given decision tree in order to predict a value. However, we still haven't discussed how to construct the decision tree. That's what we will discuss next.\n",
    "\n",
    "Scikit-Learn uses the **Classification And Regression Tree (CART)** algorithm to train Decision Trees (also called \"growing\" trees). The idea is really quite simple: the algorithm first splits the training set in two subsets using a *single* feature $k$ and a threshold $t_k$. How does it choose $k$ and $t_k$? It searches for the\n",
    "pair $(k, t_k)$ that produces the lowest weighted cost $C(k, t_k)$.\n",
    "\n",
    "$C(k, t_k) = \\frac{n_{left}}{n} C_{left} + \\frac{n_{right}}{n} C_{right}$\n",
    "\n",
    "where $C_{left}$ and $C_{right}$ is some cost function computed on the left and right subset, $n_{left}$ and $n_{right}$ denotes the number of samples contained in the left and right subset, and $n$ denotes the total number of samples.\n",
    "\n",
    "By default, Scikit uses the MSE (Mean Squared Error) as the cost function for regression problems. However, we can configure `DecisionTreeRegressor` a different metric such as the MAE (Mean Absolute Error) by setting `criterion` accordingly. Note that the choice of the splitting threshold depends on the chosen cost function. If we use the MSE as a cost function, the predicted value for a node becomes the mean of all samples in the node. However, if we use the MAE, the median of the node's samples is used. This is simply because choosing the mean value as the prediction results in the lowest possible MSE, while choosing the media value results in the lowest possible MAE.\n",
    "\n",
    "Once it has successfully split the training set in two, it splits the subsets using the\n",
    "same logic, then the sub-subsets and so on, recursively. It stops recursing once it reaches the maximum depth (defined by the `max_depth` hyperparameter), or if it cannot find a split that will reduce the costs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b3eb68",
   "metadata": {},
   "source": [
    "**The following code illustrates how to determine the threshold for the root node:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "061a00a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sort_idx = np.argsort(x, axis=0)\n",
    "x_sorted = np.take_along_axis(x, sort_idx, axis=0)\n",
    "y_sorted = np.take_along_axis(y, sort_idx, axis=0)\n",
    "\n",
    "min_weighted_mse = np.inf\n",
    "split_idx = -1\n",
    "\n",
    "for i in range(1, y_sorted.shape[0]):\n",
    "\n",
    "    # Get samples in left and right set\n",
    "    y_left = y_sorted[:i]\n",
    "    y_right = y_sorted[i:]\n",
    "    \n",
    "    # Number of samples in each set\n",
    "    num_left = len(y_left)\n",
    "    num_right = len(y_right)\n",
    "    num_total = num_left + num_right\n",
    "    \n",
    "    # Estimated target value for each set\n",
    "    pred_left = y_left.mean()\n",
    "    pred_right = y_right.mean()\n",
    "    \n",
    "    # Compute MSE for each set\n",
    "    mse_left = np.mean((y_left - pred_left) ** 2)\n",
    "    mse_right = np.mean((y_right - pred_right) ** 2)\n",
    "    \n",
    "    weighted_mse = num_left / num_total * mse_left + num_right / num_total * mse_right\n",
    "    \n",
    "    if weighted_mse < min_weighted_mse:\n",
    "        min_weighted_mse = weighted_mse\n",
    "        split_idx = i\n",
    "\n",
    "# Threshold = Value in between two samples\n",
    "threshold = (x_sorted[split_idx-1, 0] + x_sorted[split_idx, 0]) / 2\n",
    "left_pred = y_sorted[x_sorted <= threshold].mean()\n",
    "right_pred = y_sorted[x_sorted > threshold].mean()\n",
    "\n",
    "print(f'Threshold: {threshold:.3f}')\n",
    "print('Num samples in left set:', (x <= threshold).sum())\n",
    "print('Num samples in right set:', (x > threshold).sum())\n",
    "print(f'Left Child - Estimated value: {left_pred:.3f}')\n",
    "print(f'Right Child - Estimated value: {right_pred:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f94ca36",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "\n",
    "In order to explore how we can regularize decision trees, let's first add some noise to our quadratic function to make it hard for the decision tree to fit the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe91203",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "num_samples = 200\n",
    "x = (np.random.rand(num_samples, 1) - 0.5)\n",
    "noise = np.random.randn(num_samples, 1)\n",
    "y = x ** 2 + 0.025 * noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e4ad57",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x, y, s=3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064de144",
   "metadata": {},
   "source": [
    "We now test decision trees of different depths in order to see how they \"react\" to noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562dd8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.linspace(-0.5, 0.5, 1000).reshape(-1, 1)\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "\n",
    "for idx, max_depth in enumerate([3, 4, 7, 30]):\n",
    "\n",
    "    fig.add_subplot(2, 2, idx+1)\n",
    "\n",
    "    model = DecisionTreeRegressor(max_depth=max_depth)\n",
    "    \n",
    "    model.fit(x, y)\n",
    "    y_pred = model.predict(x_test)\n",
    "    \n",
    "    plt.plot(x_test, y_pred, c='r')\n",
    "    plt.scatter(x, y, s=3)\n",
    "    plt.title(f'Max depth = {max_depth}')\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7789d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question: At which tree depth do we encounter overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26459512",
   "metadata": {},
   "source": [
    "Decision Trees make very few assumptions about the training data (as opposed to linear models, which obviously assume that the data is linear, for example). If left unconstrained, the tree structure will adapt itself to the training data, fitting it very\n",
    "closely, and most likely overfitting it. Such a model is often called a **nonparametric model**, not because it does not have any parameters (it often has a lot) but because the number of parameters is not determined prior to training, so the model structure is free to stick closely to the data. In contrast, a parametric model such as a linear model has a predetermined number of parameters, so its degree of freedom is limited, reducing the risk of overfitting (but increasing the risk of underfitting).\n",
    "\n",
    "To avoid overfitting the training data, you need to **restrict the Decision Tree's freedom**\n",
    "during training. This is called **regularization**. The regularization hyperparameters depend on the algorithm used, but generally you can at least restrict the maximum depth of the Decision Tree. In Scikit-Learn, this is controlled by the `max_depth` hyperparameter (the default value is `None`, which means unlimited).\n",
    "Reducing `max_depth` will regularize the model and thus reduce the risk of overfitting.\n",
    "\n",
    "The class has a few other parameters that similarly restrict the shape of the Decision Tree: \n",
    "\n",
    "- `min_samples_split`: The minimum number of samples a node must have before it can be split\n",
    "- `min_samples_leaf`: The minimum number of samples a leaf node must have\n",
    "- `min_weight_fraction_leaf`: Same as `min_samples_leaf` but expressed as a fraction of the total number of weighted instances\n",
    "- `max_leaf_nodes`: Maximum number of leaf nodes\n",
    "- `max_features`: Maximum number of features that are evaluated for splitting at each node). Increasing `min_*` hyperparameters or reducing `max_*` hyperparameters will regularize the model.\n",
    "\n",
    "**Hint:** Another way of regularizing a decision tree is by first constructing a tree without restrictions and then removing unnecessary nodes. This principle is referred to as **pruning**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ef982f",
   "metadata": {},
   "source": [
    "**The following code block illustrates the regularization effect of `min_samples_leaf` on the model:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d17d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.linspace(-0.5, 0.5, 1000).reshape(-1, 1)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 3))\n",
    "\n",
    "# We do not restrict the maximum depth of the tree (default)\n",
    "model = DecisionTreeRegressor(max_depth=None)\n",
    "reg_model = DecisionTreeRegressor(max_depth=None, min_samples_leaf=10)\n",
    "\n",
    "model.fit(x, y)\n",
    "reg_model.fit(x, y)\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "y_reg_pred = reg_model.predict(x_test)\n",
    "\n",
    "ax[0].plot(x_test, y_pred, c='r')\n",
    "ax[1].plot(x_test, y_reg_pred, c='r')\n",
    "\n",
    "ax[0].scatter(x, y, s=3)\n",
    "ax[1].scatter(x, y, s=3)\n",
    "ax[0].set_title('No regularization')\n",
    "ax[1].set_title('min_samples_leaf = 10')\n",
    "ax[0].set_xlabel('x')\n",
    "ax[0].set_ylabel('y')\n",
    "ax[1].set_xlabel('x')\n",
    "ax[1].set_ylabel('y')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1dc1639",
   "metadata": {},
   "source": [
    "On the left, the Decision Tree is trained with the default hyperparameters (i.e., no restrictions), and on the right the Decision Tree is trained with `min_samples_leaf=10`. It is quite obvious that the model on the left is overfitting, and the model on the right will probably generalize better."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
