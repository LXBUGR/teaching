{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d312d792",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "\n",
    "Boosting (originally called hypothesis boosting) refers to ensemble methods that can combine several weak learners into a strong learner. The general idea of most boosting methods is to train predictors sequentially, each trying to correct its predecessor. There are many boosting methods available, but by far the most popular are\n",
    "**AdaBoost (short for Adaptive Boosting)** and **Gradient Boosting**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b41e63ea",
   "metadata": {},
   "source": [
    "## AdaBoost\n",
    "\n",
    "AdaBoost can be explained best in a classification setting. AdaBoost aims to create an ensemble classifier $H_T(x) =\\sum_{t=1}^T \\alpha_t h_t(x)$ where $h_t$ is some weak classifier, $T$ is the number of classifiers in the ensemble and $\\alpha_t$ is some model weight. Note that the default depth of the decision tree used in AdaBoost is 1 (see Scikit documentation).\n",
    "\n",
    "The ensemble classifier is built in an iterative fashion. In each iteration $t$, a new classifier $h_t$ is trained and used to make predictions on the training set. Then the \"correctness\" of a new classifier determines the weight $\\alpha_t$ in the ensemble. We calculate the model weight $\\alpha_t$, where $Acc_t$ is the accuracy of the t-th classifier, as follows (*):\n",
    "\n",
    "$\\alpha_t = log \\Big(\\frac{Acc_t}{1-Acc_t} \\Big)$ \n",
    "\n",
    "Afterwards, we update the sample weights of the **misclassified samples** by multiplying their current sample weight with the factor $\\frac{Acc_t}{1-Acc_t}$. The sample weight simply denotes the chance that a sample is drawn from the training set. If samples have a higher weight, these samples are more likely to be drawn from the training sample \"distribution\". As we only increase the weight of the misclassified samples, the next classifier will be \"guided\" to focus on these samples.\n",
    "\n",
    "(*) See \"log-odds function\" in order to understand what this formula represents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe58d2f",
   "metadata": {},
   "source": [
    "### Log-odds function\n",
    "\n",
    "We refer to the function $log\\big(\\frac{p(x)}{1-p(x)}\\big)$ where $p(x)$ is some probability as *log-odds* function.\n",
    "\n",
    "Note that this function is unbounded!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e007e4ef",
   "metadata": {},
   "source": [
    "![log_odds_function](imgs/log_odds_function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "162b634b",
   "metadata": {},
   "source": [
    "![Log_odds](imgs/log_odds.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b17e3c",
   "metadata": {},
   "source": [
    "**Consider the following example:**\n",
    "    \n",
    "Let's assume that we have three models that were trained to solve a binary classification problem. The first model is wrong 50% of the time, the second model is wrong 95% of the time, and the third model is right 97% of the time. In other words, the accuracy of the first, second and third model is 0.5, 0.05 and 0.97, respectively.\n",
    "\n",
    "We want to build a strong model where the prediction is obtained by a weighted vote from the three models. Thus,\n",
    "to each of the three models, we assign a score, and that is how much the vote of the model will\n",
    "count in the final vote. The question is how we should assign these scores.\n",
    "\n",
    "Obviously, the third model is very reliable because it almost always predicts the class correctly. Hence, we assign a high-positive score. Among the other two, the second model should be preferred. It is almost always wrong. Hence, we simply invert its prediction to be correct most of the time. Hence, we assign a high-negative score. The first model serves no purpose as its prediction is random. Hence, its score should be 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29458440",
   "metadata": {},
   "source": [
    "This is exactly how the model weight $\\alpha_t$ is assigned in the AdaBoost. Models that are mostly right (high accuracy) receive a high positive weight and models that are mostly wrong (low accuracy) receive a high negative weight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78e0809",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f723ab1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "import numpy as np\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb11977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample points from the moon dataset\n",
    "x, y = make_moons(n_samples=500, noise=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9334c1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Train an ADABoost classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09af17cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(clf, X, y, alpha=1.0):\n",
    "    \n",
    "    axes=[-1.5, 2.4, -1, 1.5]\n",
    "    x1, x2 = np.meshgrid(np.linspace(axes[0], axes[1], 100),\n",
    "                         np.linspace(axes[2], axes[3], 100))\n",
    "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
    "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
    "    \n",
    "    plt.contourf(x1, x2, y_pred, alpha=0.3 * alpha, cmap='Wistia')\n",
    "    plt.contour(x1, x2, y_pred, cmap=\"Greys\", alpha=0.8 * alpha)\n",
    "    colors = [\"#78785c\", \"#c47b27\"]\n",
    "    markers = (\"o\", \"^\")\n",
    "    \n",
    "    for idx in (0, 1):\n",
    "        plt.plot(X[:, 0][y == idx], X[:, 1][y == idx],\n",
    "                 color=colors[idx], marker=markers[idx], linestyle=\"none\")\n",
    "        \n",
    "    plt.axis(axes)\n",
    "    plt.xlabel(r\"$x_1$\")\n",
    "    plt.ylabel(r\"$x_2$\", rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abc7734",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plot_decision_boundary(classifier, x, y)\n",
    "plt.title(\"AdaBoost Classifier\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c1a57a",
   "metadata": {},
   "source": [
    "## AdaBoost for Regression\n",
    "\n",
    "Not discussed. See https://dafriedman97.github.io/mlbook/content/c6/s2/boosting.html for further details."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
