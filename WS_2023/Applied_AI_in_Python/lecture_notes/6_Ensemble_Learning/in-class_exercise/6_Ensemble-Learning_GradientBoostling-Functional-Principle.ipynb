{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d312d792",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "\n",
    "Boosting (originally called hypothesis boosting) refers to ensemble methods that can combine several weak learners into a strong learner. The general idea of most boosting methods is to train predictors sequentially, each trying to correct its predecessor. There are many boosting methods available, but by far the most popular are\n",
    "**AdaBoost (short for Adaptive Boosting)** and **Gradient Boosting**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e135ec84",
   "metadata": {},
   "source": [
    "## Gradient Boosting\n",
    "\n",
    "Gradient boosting is similar to AdaBoost, in that the weak learners are decision trees, and the goal of each weak learner is to learn from the mistakes of the previous ones. One difference between gradient boosting and AdaBoost is that in gradient boosting, we allow decision trees of depth greater than 1. Gradient boosting can be used for regression and classification, but for clarity, we use a regression example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba29b0c1",
   "metadata": {},
   "source": [
    "Let's assume we are given the following dataset composed of eight samples with a single feature and target label per sample. Our goal is to use gradient boosting to predict the target label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4994416b",
   "metadata": {},
   "source": [
    "![GradientBoosting_Dataset](imgs/gradient_boosting_dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e73e45c",
   "metadata": {},
   "source": [
    "The idea of gradient boosting is to create a sequence of trees that fit the dataset. The main hyperparameters of a Gradient Boosting model are the number of trees, the learning rate and the maximum depth of a tree. For this example, we assume that the number of trees is 5, the learning rate is 0.8 and the maximum model depth is 2. Since we are dealing with a regression problem, we aim to minimize the MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078839e6",
   "metadata": {},
   "source": [
    "**1st step:** Create a tree with depth 0. Because the error function we are minimizing is the mean square error, the optimal value for the prediction is the average value of the labels. We now calculate the residual, which is the difference between the label and the prediction made by this first weak learner, and fit a new decision tree to these residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24821c80",
   "metadata": {},
   "source": [
    "In other words, the prediction of the first tree looks a follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c95913",
   "metadata": {},
   "source": [
    "![GradientBoosting_Pred_1](imgs/gradient_boosting_tree1_result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fc5788",
   "metadata": {},
   "source": [
    "**2nd step:** We now train the second decision tree on the residuals.\n",
    "\n",
    "The second decision tree looks as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bb1435",
   "metadata": {},
   "source": [
    "![GradientBoosting_Tree2](imgs/gradient_boosting_tree2.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2b2c26",
   "metadata": {},
   "source": [
    "![GradientBoosting_Tree2_result](imgs/gradient_boosting_tree2_result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bc5f131",
   "metadata": {},
   "source": [
    "We now combine the predictions of the first and second learner by adding them. However, before doing that, the prediction of the second learner gets multiplied by the learning rate (=0.8). This is because we don't want to overfit by fitting our training data too well. Our goal is to mimic the gradient descent algorithm by slowly walking closer and closer to the solution.\n",
    "\n",
    "Afterwards, we again calculate the residual between the label and the combined prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcf13ab",
   "metadata": {},
   "source": [
    "![GradientBoosting_combined_result](imgs/gradient_boosting_tree_combined_result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3c5e59",
   "metadata": {},
   "source": [
    "**3rd step**: We repeat this process. More precisely, we fit a new weak learner on the new residuals and calculate the combined prediction of the first two weak learners. We obtain this by adding the prediction for the first weak learner and 0.8 (the learning rate) times the sum of the predictions of the second and third weak learner. We repeat this process for every weak learner we want to build."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268a0760",
   "metadata": {},
   "source": [
    "![GradientBoosting_Overview](imgs/gradient_boosting_overview.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
