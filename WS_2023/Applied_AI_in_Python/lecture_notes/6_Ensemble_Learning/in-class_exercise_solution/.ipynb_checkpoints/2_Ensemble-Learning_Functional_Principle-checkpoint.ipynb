{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d312d792",
   "metadata": {},
   "source": [
    "# Ensemble Learning\n",
    "\n",
    "Classifiers such as Random Forest classifiers fall into the category of **ensemble methods**.\n",
    "\n",
    "The approach followed by ensemble methods is simple: Instead of relying on the output of a single model, we **combine the output of multiple models** to obtain better predictive performance than with a single classifier / regressor.\n",
    "\n",
    "To understand why this often results in better predictive performance, we can think of the following analogy: <br/>\n",
    "Suppose you ask a complex question to thousands of random people, then aggregate\n",
    "their answers. Often, you will find that this aggregated answer is better than\n",
    "an expert's answer. This is called the *wisdom of the crowd*. \n",
    "\n",
    "Similarly, if we aggregate the predictions of a group of predictors (such as classifiers or regressors), we might get better predictions than with the best individual predictor. A group of predictors is called an ensemble. This technique is called Ensemble Learning, and an Ensemble Learning algorithm is called an Ensemble method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "762671f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea3f17e",
   "metadata": {},
   "source": [
    "## Voting Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1031da",
   "metadata": {},
   "source": [
    "Suppose you have trained a few classifiers, each one achieving about 80\\% accuracy.\n",
    "You may have a Logistic Regression classifier, an SVM classifier, a Random Forest classifier, etc.."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fba88a",
   "metadata": {},
   "source": [
    "![Ensemble_Method](img/ensemble_method.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0ddeca",
   "metadata": {},
   "source": [
    "A very simple way to create an even better classifier is to aggregate the predictions of\n",
    "each classifier and predict the class that gets the most votes. This **majority-vote classifier** is called a **hard voting classifier**. Alternatively, we can also use a **soft-voting classifier**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab323b3",
   "metadata": {},
   "source": [
    "- **Hard voting (also known as majority voting)**: Every individual classifier votes for a class, and the majority wins\n",
    "- **Soft voting:** In soft voting, every individual classifier provides a probability value that a specific data point belongs to a particular target class. The predictions are weighted by the classifier\\'s importance and summed up. Then the target label with the greatest sum of weighted probabilities wins the vote."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d00e57",
   "metadata": {},
   "source": [
    "**Example:** Let's assume we trained three classifiers to predict whether a given e-mail is spam or not. According to classifier A, there is a 65% chance that the e-mail is spam. Classifier B predicts it's 45\\% and classifier C predicts it's 49\\%.\n",
    "\n",
    "- **Hard Voting:** If we assume that everything greater than 50\\% is spam, 1 out of 3 classifiers predicts that the e-mails is spam. Hence, according to the hard-voting classifier, the e-mail isn't spam.\n",
    "\n",
    "- **Soft Voting:** We assume that every classifier has equal importance. Averaging the probabilities results in a 53\\% chance that the e-mail is spam. Hence, our final decision is that the e-mail is spam."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb03f494",
   "metadata": {},
   "source": [
    "Somewhat surprisingly, this voting classifiers often achieve higher accuracy than the best classifier in the ensemble. In fact, even if each classifier is a weak learner (meaning it does only slightly better than random guessing), the ensemble can still be a strong learner (achieving high accuracy), provided there are a sufficient number of weak learners and they are sufficiently diverse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce183127",
   "metadata": {},
   "source": [
    "## Bagging (Bootstrap Aggregation)\n",
    "\n",
    "One way to get a diverse set of classifiers / regressors is to use different training algorithms, as just discussed. Another approach is to use the same training algorithm for every predictor, but to **train them on different random subsets** of the training set. When **sampling** is performed **with replacement**, this method is called **bagging** (short for bootstrap (*) aggregating). When sampling is performed without replacement, it is called pasting.\n",
    "\n",
    "To implement baggging, we first sample $n$ random subsets with replacement from the training set. We then train a model on each individual subset and obtain an ensemble of $n$ models. Finally, assuming that we deal with a regression problem, we obtain the ensemble's prediction by averaging the predictions of the individual model. If we are dealing with a classification problem, we can apply majority voting.\n",
    "\n",
    "(*) In statistics, resampling with replacement is called bootstrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71604739",
   "metadata": {},
   "source": [
    "![Bootstrapping](img/bootstrapping.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9393e9c",
   "metadata": {},
   "source": [
    "**Bagging can effectively reduce the variance of an model**, so it has strong beneficial effects on high variance models (*)\n",
    "\n",
    "(*) Models that have many parameters and are likely to perfectly fit the training data. High variance models are prone to overfitting. An example of a low-variance model is a linear model. A model that fits a higher-degree polynomial tends to be a high-variance model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f430781d",
   "metadata": {},
   "source": [
    "### Bias - Variance Tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aeef301",
   "metadata": {},
   "source": [
    "#### Explanation of the term \"model variance\"\n",
    "\n",
    "We just stated that bagging is able to reduce the variance of a model. However, so far, we haven't really discussed what we mean when talking about the variance of a model. To get some understanding, let's take a look at the following example.\n",
    "\n",
    "Let's assume we are given some function $h(x)=sin(x)$. We sample 50 datasets from this function with 20 points each.\n",
    "Next, we fit a higher-degree polynomial(degree=6) on each individual dataset. The resulting polynomials are shown in the left plot. As can be seen, the shape of the fitted polynomial varies strongly depending on the samples in the dataset. In other words, for a given point $x$, the predicted value $y$ varies strongly for different polynomials. Hence, we say that the model has high variance. \n",
    "\n",
    "Following the idea of ensemble learning, we then try to aggregate the output of the predictors (polynomial models) by simply computing the mean y-value of all predictors estimated for a point $x$. For visualization purposes only, we repeat the entire experiment 100 times producing 100 ensemble models. The predictions of these 100 ensemble models are shown in the right plot. As can be seen, the outputs of the ensemble models for a given point $x$ hardly differ from each other. As a result, we say that the model has low variance.\n",
    "\n",
    "We can also say that the variance captures how much your classifier changes if we train on a different training set. It answers the question how \"over-specialized\" is your classifier for a particular training set (overfitting)? If we have the best possible model for our training data, how far off is it from the average classifier? \n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$Var[\\hat{h}] = \\mathbb{E}[(\\hat{h}(x) - \\mathbb{E}[\\hat{h}(x)])^2]$ where $\\hat{h}$ is a model trained on a randomly drawn set of samples.\n",
    "\n",
    "**How can we reduce the variance of a model?**\n",
    "- Increase the number training samples\n",
    "- Decrease the complexity of the model (e.g., decrease the degree of the polynomial). However, this will increase the **model bias**.\n",
    "- Apply ensemble learning (e.g., bagging)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa339e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "x_test = np.linspace(0, 2*np.pi, 500).reshape(-1, 1)\n",
    "y_test = np.sin(x_test)\n",
    "\n",
    "poly_reg = LinearRegression()\n",
    "poly_features = PolynomialFeatures(degree=6, include_bias=False)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "num_ensembles = 100\n",
    "num_models_per_ensemble = 50\n",
    "num_training_samples = 20\n",
    "\n",
    "for _ in range(num_ensembles):\n",
    "    \n",
    "    y_test_all_preds = None\n",
    "    \n",
    "    for _ in range(num_models_per_ensemble):\n",
    "\n",
    "        x_train = np.random.uniform(0, 2*np.pi, num_training_samples).reshape(-1, 1)\n",
    "        y_train = np.sin(x_train)\n",
    "\n",
    "        x_train_poly = poly_features.fit_transform(x_train)\n",
    "        x_test_poly = poly_features.fit_transform(x_test)\n",
    "\n",
    "        poly_reg.fit(x_train_poly, y_train)\n",
    "\n",
    "        y_test_pred = poly_reg.predict(x_test_poly)\n",
    "\n",
    "        y_test_all_preds = y_test_pred if y_test_all_preds is None else np.concatenate((y_test_all_preds, y_test_pred), axis=1)\n",
    "\n",
    "        ax[0].plot(x_test, y_test_pred)\n",
    "\n",
    "    # Compute the prediction of the ensemble\n",
    "    y_test_pred_ensemble = np.mean(y_test_all_preds, axis=1)\n",
    "\n",
    "    ax[1].plot(x_test, y_test, c='g')\n",
    "    ax[1].plot(x_test, y_test_pred_ensemble)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a7edea2",
   "metadata": {},
   "source": [
    "#### Explanation of the term \"model variance\"\n",
    "\n",
    "We just mentioned that reducing the complexity of a model tends to increase the model bias. The bias of an estimator is the \"expected\" difference between its estimates and the true value. \n",
    "\n",
    "Again, let's assume that we have some higher-degree polynomial. If given a small set of training samples, the model is able to \"memorize\" the data by simply choosing a polynomial that goes through all training points. However, if we have a lower-degree polynomial (e.g., deg=1 or 2), the polynomial model won't be able to fit the training and testing data perfectly. (*) The model bias aims to quantify the inability of the model to predict the right value caused by invalid assumptions made about the data. (x) We could also say that it's the error the model would make even when it's trained on infinity training data. The bias is inherent to the model.\n",
    "\n",
    "A high-bias model typically tends to underfit training data.\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "$Bias^2[\\hat{h}] = (\\mathbb{E}[\\hat{h}(x)] - h(x))^2$ where $\\hat{h}$ is a model trained on a randomly drawn set of samples and $h(x)$ is the true value.\n",
    "\n",
    "(*) https://rasbt.github.io/mlxtend/user_guide/evaluate/bias_variance_decomp/ provides a nice visualization\n",
    "(x) e.g., we assume that there is some linear relationship but there's no linear relationship"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096fef69",
   "metadata": {},
   "source": [
    "#### Bias - Variance Tradeoff\n",
    "\n",
    "Obviously, the ideal model has low variance and low bias. Unfortunately, it turns out that it is not possible to achieve both. Once we increase the model complexity to reduce the model bias, the model variance increases. This behavior is referred to as the bias-variance tradeoff.\n",
    "\n",
    "See also http://scott.fortmann-roe.com/docs/BiasVariance.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29e6a5c",
   "metadata": {},
   "source": [
    "![bias_variance_tradeoff](img/bias_variance_tradeoff.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5434dee2",
   "metadata": {},
   "source": [
    "### Out-of-bag evaluation\n",
    "\n",
    "With bagging, some instances may be sampled several times for any given predictor,\n",
    "while others may not be sampled at all. If you randomly draw one instance from a dataset of size $m$, each instance in the dataset obviously has probability $\\frac{1}{m}$ of getting picked, and therefore it has a probability $1 - \\frac{1}{m}$ of not getting picked. If you draw $m$ instances with replacement, all draws are independent and therefore each instance has a probability $(1 - \\frac{1}{m})^m$  of not getting picked. Now notice that $e^x = \\lim\\limits_{m\\to \\infty} (1+\\frac{x}{m})^m$. So if $m$ is large, the ratio of out-of-bag instances will be about $e^{-1} = 0.37$. So roughly $63\\%$ will be sampled."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1aa93dc",
   "metadata": {},
   "source": [
    "In theory, since a predictor never sees the out-of-bag instances during training, it can be evaluated on\n",
    "these instances, without the need for a separate validation set or cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc49079",
   "metadata": {},
   "source": [
    "### Random Forest Classifiers\n",
    "\n",
    "A Random Forest is an ensemble of Decision Trees, generally trained via the bagging method (or sometimes pasting), typically with `max_samples` set to the size of the training set. Note that the Random Forest algorithm introduces extra randomness when growing trees; instead of searching for the very best feature when splitting a node, it searches for the best feature among a **random subset of features**. This can be controlled via the `max_features` parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e4046e",
   "metadata": {},
   "source": [
    "The following example illustrates the performance of a random forest classifier compared to a decision tree classifier on the moon dataset. The moon dataset is a simple toy dataset often used to visualize clustering and classification algorithms. The generated points look like two interleaving half circles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b66635ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample points from the moon dataset\n",
    "x, y = make_moons(n_samples=500, noise=0.30, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c1faba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a decision tree as well as a random forest classifier to classify the moon dataset\n",
    "\n",
    "decision_tree_classifier = DecisionTreeClassifier(max_depth=None)\n",
    "random_forest_classifier = RandomForestClassifier(\n",
    "    n_estimators=100, # Default\n",
    "    bootstrap=True, # Default\n",
    "    max_depth=None, # Default\n",
    "    max_features=None, # Default: max_features=sqrt(num_features)\n",
    "    max_samples=None # Default. We draw x.shape[0] samples.\n",
    ")\n",
    "\n",
    "\n",
    "decision_tree_classifier.fit(x, y)\n",
    "random_forest_classifier.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f38ec5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(clf, X, y, alpha=1.0):\n",
    "    \n",
    "    axes=[-1.5, 2.4, -1, 1.5]\n",
    "    x1, x2 = np.meshgrid(np.linspace(axes[0], axes[1], 100),\n",
    "                         np.linspace(axes[2], axes[3], 100))\n",
    "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
    "    y_pred = clf.predict(X_new).reshape(x1.shape)\n",
    "    \n",
    "    plt.contourf(x1, x2, y_pred, alpha=0.3 * alpha, cmap='Wistia')\n",
    "    plt.contour(x1, x2, y_pred, cmap=\"Greys\", alpha=0.8 * alpha)\n",
    "    colors = [\"#78785c\", \"#c47b27\"]\n",
    "    markers = (\"o\", \"^\")\n",
    "    \n",
    "    for idx in (0, 1):\n",
    "        plt.plot(X[:, 0][y == idx], X[:, 1][y == idx],\n",
    "                 color=colors[idx], marker=markers[idx], linestyle=\"none\")\n",
    "        \n",
    "    plt.axis(axes)\n",
    "    plt.xlabel(r\"$x_1$\")\n",
    "    plt.ylabel(r\"$x_2$\", rotation=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec01b872",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
    "plt.sca(axes[0])\n",
    "plot_decision_boundary(decision_tree_classifier, x, y)\n",
    "plt.title(\"Decision Tree\")\n",
    "plt.sca(axes[1])\n",
    "plot_decision_boundary(random_forest_classifier, x, y)\n",
    "plt.title(\"Random Forest\")\n",
    "plt.ylabel(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4014e8b9",
   "metadata": {},
   "source": [
    "## Boosting\n",
    "\n",
    "Boosting (originally called hypothesis boosting) refers to ensemble methods that can combine several weak learners into a strong learner. The general idea of most boosting methods is to train predictors sequentially, each trying to correct its predecessor. There are many boosting methods available, but by far the most popular are\n",
    "AdaBoost (short for Adaptive Boosting) and Gradient Boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31354395",
   "metadata": {},
   "source": [
    "### AdaBoost\n",
    "\n",
    "AdaBoost can be explained best in a classification setting. AdaBoost aims to create an ensemble classifier $H_T(x) =\\sum_{t=1}^T \\alpha_t h_t(x)$ where $h_t$ is some weak classifier, $T$ is the number of classifiers in the ensemble and $\\alpha_t$ is some model weight. Note that the default depth of the decision trees classifier used in AdaBoost is 1.\n",
    "\n",
    "The ensemble classifier is built in an iterative fashion. In each iteration $t$, a new classifier $h_t$ is trained and used to make predictions on the training set. Then the \"correctness\" of a new classifier determines the weight $\\alpha_t$ in the ensemble. We calculate the model weight $\\alpha_t$ where $Acc_t$ is the accuracy of the new classifier as follows:\n",
    "\n",
    "$\\alpha_t = log \\Big(\\frac{Acc_t}{1-Acc_t} \\Big)$ \n",
    "\n",
    "Afterwards, we update the sample weights of the **misclassified samples** by multiplying their current sample weight with the factor $\\frac{Acc_t}{1-Acc_t}$. The sample weight simply denotes the chance that a sample is drawn from the training set. If samples have a higher weight, these samples are more likely to be drawn from the training sample \"distribution\". As we only increase the weight of the misclassified samples, the next classifier will be \"guided\" to focus on these samples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0dd94b",
   "metadata": {},
   "source": [
    "**Log-odds function:**\n",
    "\n",
    "We refer to the function $log\\big(\\frac{p(x)}{1-p(x)}\\big)$ where $p(x)$ is some probability as *log-odds* function.\n",
    "\n",
    "Note that this function is unbounded!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c79509",
   "metadata": {},
   "source": [
    "![log_odds_function](img/log_odds_function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d19ebf1",
   "metadata": {},
   "source": [
    "![Log_odds](img/log_odds.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c703705",
   "metadata": {},
   "source": [
    "**Consider the following example:**\n",
    "    \n",
    "Let's assume that we have three models that were trained to solve a binary classification problem. The first model is wrong 50% of the time, the second model is wrong 95% of the time, and the third model is right 97% of the time. In other words, the accuracy of the first, second and third model is 0.5, 0.05 and 0.97, respectively.\n",
    "\n",
    "We want to build a strong model where the prediction is obtained by a weighted vote from the three models. Thus,\n",
    "to each of the three models, we assign a score, and that is how much the vote of the model will\n",
    "count in the final vote. The question is how we should assign these scores.\n",
    "\n",
    "Obviously, the third model is very reliable because it almost always predicts the class correctly. Hence, we assign a high-positive score. Among the other two, the second model should be preferred. It is almost always wrong. Hence, we simply invert its prediction to be correct most of the time. Hence, we assign a high-negative score. The first model serves no purpose as its prediction is random. Hence, its score should be 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8fc686",
   "metadata": {},
   "source": [
    "This is exactly how the model weight $\\alpha_t$ is assigned in the AdaBoost. Models that are mostly right (high accuracy) receive a high positive weight and models that are mostly wrong (low accuracy) receive a high negative weight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8bdd4a",
   "metadata": {},
   "source": [
    "### AdaBoost for Regression\n",
    "\n",
    "Not discussed. See https://dafriedman97.github.io/mlbook/content/c6/s2/boosting.html for further details."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84b5ca6b",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "\n",
    "Gradient boosting is similar to AdaBoost, in that the weak learners are decision trees, and the goal of each weak learner is to learn from the mistakes of the previous ones. One difference between gradient boosting and AdaBoost is that in gradient boosting, we allow decision trees of depth greater than 1. Gradient boosting can be used for regression and classification, but for clarity, we use a regression example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9695ff",
   "metadata": {},
   "source": [
    "Let's assume we are given the following dataset composed of eight samples with a single feature and target label per sample. Our goal is to use gradient boosting to predict the target label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158d9f35",
   "metadata": {},
   "source": [
    "![GradientBoosting_Dataset](img/gradient_boosting_dataset.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba59270c",
   "metadata": {},
   "source": [
    "The idea of gradient boosting is to create a sequence of trees that fit the dataset. The main hyperparameters of a Gradient Boosting model are the number of trees, the learning rate and the maximum depth of a tree. For this example, we assume that the number of trees is 5, the learning rate is 0.8 and the maximum model depth is 2. Since we are dealing with a regression problem, we aim to minimize the MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef098ed",
   "metadata": {},
   "source": [
    "**1st step:** Create a tree with depth 0. Because the error function we are minimizing is the mean square error, the optimal value for the prediction is the average value of the labels. We now calculate the residual, which is the difference between the label and the prediction made by this first weak learner, and fit a new decision tree to these residuals."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec785ee",
   "metadata": {},
   "source": [
    "In other words, the prediction of the first tree looks a follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d23500",
   "metadata": {},
   "source": [
    "![GradientBoosting_Pred_1](img/gradient_boosting_tree1_result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c22f110f",
   "metadata": {},
   "source": [
    "**2nd step:** We now train the second decision tree on the residuals.\n",
    "\n",
    "The second decision tree looks as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e6302a",
   "metadata": {},
   "source": [
    "![GradientBoosting_Tree2](img/gradient_boosting_tree2.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d321c7",
   "metadata": {},
   "source": [
    "![GradientBoosting_Tree2_result](img/gradient_boosting_tree2_result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cb1463",
   "metadata": {},
   "source": [
    "We now combine the predictions of the first and second learner by adding them. However, before doing that, the prediction of the second learner gets multiplied by the learning rate (=0.8). This is because we don't want to overfit by fitting our training data too well. Our goal is to mimic the gradient descent algorithm by slowly walking closer and closer to the solution.\n",
    "\n",
    "Afterwards, we again calculate the residual between the label and the combined prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd7b772",
   "metadata": {},
   "source": [
    "![GradientBoosting_combined_result](img/gradient_boosting_tree_combined_result.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c89319",
   "metadata": {},
   "source": [
    "**3rd step**: We repeat this process. More precisely, we fit a new weak learner on the new residuals and calculate the combined prediction of the first two weak learners. We obtain this by adding the prediction for the first weak learner and 0.8 (the learning rate) times the sum of the predictions of the second and third weak learner. We repeat this process for every weak learner we want to build."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cc29af",
   "metadata": {},
   "source": [
    "![GradientBoosting_Overview](img/gradient_boosting_overview.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db2d427",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
