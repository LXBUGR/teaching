{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d312d792",
   "metadata": {},
   "source": [
    "# Neural Networks: Introduction to FCNs\n",
    "\n",
    "In this tutorial, we will train our first neural network (NN) using PyTorch. Like in the previous examples, our goal is to predict the housing prices in the California Housing dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69448b7",
   "metadata": {},
   "source": [
    "## Basic imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a127b986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2603675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To ensure reproducability\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5954d4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70015490",
   "metadata": {},
   "source": [
    "## Fetch the california housing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f82cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fetch_california_housing(as_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c083760",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df = dataset['data']\n",
    "target_df = dataset['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dbddf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert the housing prices in the housing df\n",
    "housing_df['HousePrice'] = target_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56665718",
   "metadata": {},
   "source": [
    "## Prepare the training and testing set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b4cbdc",
   "metadata": {},
   "source": [
    "### Splitting the dataframe (using sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d051252",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(housing_df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9518d555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Briefly check whether we have the correct set sizes\n",
    "print('Test ratio: ', len(test_df) / (len(train_df) + len(test_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3bebe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
    "target_column = 'HousePrice'\n",
    "\n",
    "x_train = train_df[feature_columns].values\n",
    "y_train = train_df[['HousePrice']].values\n",
    "\n",
    "x_test = test_df[feature_columns].values\n",
    "y_test = test_df[['HousePrice']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ededd5ea",
   "metadata": {},
   "source": [
    "### Build a neural network with PyTorch\n",
    "\n",
    "We will now build our first neural network that is able to predict housing prices based on the eight features provided in the dataset.\n",
    "\n",
    "The network architecture should look as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ba2c29",
   "metadata": {},
   "source": [
    "<img src=\"imgs/linear_model_1.png\" width=\"600px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a66bf0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38735769",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dims = len(feature_columns)\n",
    "output_dims = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e230c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(input_dims, 256),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(256, 128),\n",
    "    nn.Sigmoid(),\n",
    "    nn.Linear(128, 1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0919efe5",
   "metadata": {},
   "source": [
    "We can now print the network to verify the network architecture is correct ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9bee96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1b5028",
   "metadata": {},
   "source": [
    "Looking good. Next, let's create some random input sample (tensor) and feed it through the network in order to see how the network's output looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f80f1cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6631d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (1, len(feature_columns)) # (NUM_SAMPLES, NUM_FEATURES)\n",
    "x = torch.rand(shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f97fd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.shape)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f0a04ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c8cc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pred.shape)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b9058d",
   "metadata": {},
   "source": [
    "## Preprocess the samples "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7553ab8",
   "metadata": {},
   "source": [
    "Before training the neural network, input features should be normalized (very often to the range -1 to 1) or standardized, and converted to a PyTorch tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c787e717",
   "metadata": {},
   "source": [
    "### Standardize the input features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e976c3b3",
   "metadata": {},
   "source": [
    "In this example, we will use Scikit's StandardScaler to standardized the input features. In future examples, we will start to use PyTorch's \"on-board capabilities\" to preprecess the input features (e.g., https://pytorch.org/vision/main/generated/torchvision.transforms.Normalize.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daf0e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac7243a",
   "metadata": {},
   "source": [
    "### Convert all samples to a PyTorch tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbaccbb",
   "metadata": {},
   "source": [
    "The entire dataset (`x_train` and `x_test`) is stored as numpy array at this point and we still have convert it to a PyTorch tensor.\n",
    "\n",
    "Tensors are similar to numpy ndarrays, except that tensors can run on GPUs or other hardware accelerators. In fact, tensors and NumPy arrays can often share the same underlying memory, eliminating the need to copy data (see Bridge with NumPy). Tensors are also optimized for automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58fb2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be63e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.tensor(x_train, dtype=torch.float32)\n",
    "x_test = torch.tensor(x_test, dtype=torch.float32)\n",
    "\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01792eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(type(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4f5d3f",
   "metadata": {},
   "source": [
    "## Train the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae450929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix seed to ensure reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27642787",
   "metadata": {},
   "source": [
    "### Helper function that computes for RMSE for a given dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242d024e",
   "metadata": {},
   "source": [
    "The *evaluate_model_performance()* function receives a set of features and their target levels as its input, and evaluates the model performance using the root mean square error. In order to do this, the network must be switched to *inference mode* (`model.eval()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e220a7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_performance(x, y, batch_size=128):\n",
    "    \n",
    "    assert x.shape[0] == y.shape[0], 'Feature and target labels a different number of samples'\n",
    "    \n",
    "    # During training, the batch norm layer keeps a running estimate of its computed mean and variance. \n",
    "    # At inference time, the mean/variance must not be updated. The estimed mean/variance is used for normalization.\n",
    "    model.eval()\n",
    "    \n",
    "    total_error = 0.\n",
    "    \n",
    "    num_samples = x.shape[0]\n",
    "    \n",
    "    # No gradient have to be computed at inference time. We can disable gradient computation by means of the\n",
    "    # no_grad() context manager. This will reduce the memory consumption.\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for indices in batch(range(num_samples), batch_size):\n",
    "            \n",
    "            # Convert the sample to a Pytorch tensor\n",
    "            sample_x = x[indices]\n",
    "            sample_y = y[indices]\n",
    "            \n",
    "            pred = model(sample_x)\n",
    "            \n",
    "            total_error += torch.sum((pred - sample_y) **2)\n",
    "            \n",
    "    rmse = torch.sqrt(total_error / num_samples) * 100000\n",
    "    \n",
    "    # Switch the network back to training mode since want to continue training.\n",
    "    model.train()\n",
    " \n",
    "    return rmse.numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cacde1e",
   "metadata": {},
   "source": [
    "### Training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60921ad",
   "metadata": {},
   "source": [
    "The training procedure for a neural network looks as follows: First, sample a random set of samples from the training set. This randomly drawn subset is referred to as a **batch**.\n",
    "\n",
    "The training batch is fed through the network to obtain the prediction (output of the network). Next, we need to update the parameters (weights) of the network. To accomplish this, we first have to compute the loss (a.k.a. cost function) and then compute the gradient with respect to every parameter in the network. Once we obtain the individual gradients, we can update the parameters. For updating the parameters, we use an optimizer such as ADAM or SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e82d4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a195986c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_training_iterations = 13000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c953c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the ADAM optimizer to find the \"optimal\" network weights.\n",
    "# The ADAM optimization is a stochastic gradient descent method that is based on \n",
    "# adaptive estimation of first-order and second-order moments.\n",
    "optimizer = Adam(model.parameters(), lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9185ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rmse = []\n",
    "\n",
    "for batch_idx in trange(num_training_iterations):\n",
    "    \n",
    "    # Clear all gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Draw a random batch of samples from the training set\n",
    "    batch_indices = np.random.randint(0, x_train.shape[0], size=batch_size)\n",
    "    batch_x = x_train[batch_indices]\n",
    "    batch_y = y_train[batch_indices]\n",
    "\n",
    "    # Push the training batch through the neural network\n",
    "    pred = model(batch_x)\n",
    "    \n",
    "    # Compute the mean squared error --> Our goal is to minimize this error\n",
    "    loss = F.mse_loss(pred, batch_y)\n",
    "    \n",
    "    # Trigger backpropagation. This computes all gradients that have to be optimized.\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the weights based on the obtained gradient values\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Hint: We can view weights/gradients of a linear layer as follows:  \n",
    "    #print(model[0].weight)\n",
    "    #print(model[0].weight.grad)\n",
    "    #print(model[0].bias)\n",
    "    #print(model[0].bias.grad)\n",
    "\n",
    "    if batch_idx % 1000 == 0:\n",
    "        train_rmse.append((batch_idx, evaluate_model_performance(x_train, y_train)))\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d664d7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = list(zip(*train_rmse))\n",
    "plt.plot(x, y)\n",
    "plt.title('Train RMSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df0608a",
   "metadata": {},
   "source": [
    "#### Defining the training duration via its epochs\n",
    "\n",
    "In the previous section, we introduced the concept of 'iterations' in the context of training a neural network. Specifically, a 'training iteration' is a discrete step in the training process during which the model's parameters, namely weights and biases, are updated to improve its performance. During each iteration, we randomly select a subset of the training data, known as a 'batch', and use this batch to refine the network's parameters. By changing the number of iterations, we controlled the training duration.\n",
    "\n",
    "In PyTorch, the training duration is typically specified by setting the number of training epochs. An **epoch** is defined as **one complete traversal through the entire training dataset**, where **the neural network processes each sample once**. \n",
    "\n",
    "The relationship between epochs and iterations is intricately linked to the size of the training dataset and the batch size. The batch size determines how many data samples the network processes in a single iteration. For instance, if your training dataset comprises 1000 samples and you set a batch size of 100, it will require 10 iterations (batches) to process the entire dataset, thereby completing one epoch. This is because the 1000 samples are divided into 10 batches of 100 samples each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8129d814",
   "metadata": {},
   "source": [
    "Let's take a closer specifying training via epochs looks like in practice ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0a5982",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "num_samples = x_train.shape[0]\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3223a692",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Samples in the training set:', num_samples)\n",
    "print('Batch size:', batch_size)\n",
    "print(f'Batches per epoch (a.k.a. iterations per epoch): {num_samples/batch_size=}')\n",
    "print(f'Batches/Iterations per training run: {(num_samples/batch_size)*num_epochs=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c3fd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in trange(num_epochs):\n",
    "    \n",
    "    # Samples should be drawn in random order.\n",
    "    # Hence, we create a list of indices (in random order) from which we can sample from.\n",
    "    shuffled_indices = list(range(num_samples))\n",
    "    random.shuffle(shuffled_indices)\n",
    "    \n",
    "    batch_iter = batch(shuffled_indices, 128)\n",
    "    \n",
    "    for batch_idx, sample_indices in enumerate(batch_iter):\n",
    "\n",
    "        # Clear all gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_x = x_train[sample_indices]\n",
    "        batch_y = y_train[sample_indices]\n",
    "\n",
    "        # Push the training batch through the neural network\n",
    "        pred = model(batch_x)\n",
    "\n",
    "        # Compute the mean squared error --> Our goal is to minimize this error\n",
    "        loss = F.mse_loss(pred, batch_y)\n",
    "\n",
    "        # Trigger backpropagation. This computes all gradients that have to be optimized.\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights based on the obtained gradient values\n",
    "        optimizer.step()\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c5da03",
   "metadata": {},
   "source": [
    "### Performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe35647",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rmse = evaluate_model_performance(x_train, y_train)\n",
    "test_rmse = evaluate_model_performance(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85938a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train RMSE:', train_rmse)\n",
    "print('Test RMSE:', test_rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad1e3ef",
   "metadata": {},
   "source": [
    "Congrats, you just trained our first neural network! As you can see, the RMSE obtained on the test set isn't great. In order to reduce the RMSE, we have to alter the architecture (add more layers, change the activation function, etc.)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
