{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d312d792",
   "metadata": {},
   "source": [
    "# Neural Networks: BatchNorm + ReLU\n",
    "\n",
    "In the previous tutorial, we constructed our first neural network composed of linear layers and sigmoid activations. In this notebook, we will try to further improve the network by replacing the sigmoid with a ReLU activation and by adding batch normalization layers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce6f58f",
   "metadata": {},
   "source": [
    "### Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdf083b",
   "metadata": {},
   "source": [
    "#### Disadvantrages of the sigmoid activation\n",
    "So far, we have only used the sigmoid activation to enable non-linear mappings. However, in state-of-the-art (SOTA) networks, sigmoid activation can hardly be found as better activation functions have been identified.\n",
    "\n",
    "Looking at the graph below, we can see the sigmoid activation function (left) and the derivative of the function (right). As can be seen, the maximum of the derivative is 0.25, and the derivative becomes almost zero for large $x$  (i.e., > 5) or small values (i.e., < -5). \n",
    "\n",
    "<img src=\"imgs/sigmoid.png\" width=\"400px\" />\n",
    "\n",
    "Since backpropagation utilizes on the derivative of the function for updating the weights, hardly an change will happen for large/small $x$ values. One we enter this area, the gradient vanishes and training gets stuck. <br/>\n",
    "Furthermore, considering that the maximum derivative is 0.25, having multiple layers with sigmoid activation, will cause the gradient to become smaller. For example, after two layers the maximum would be ($\\frac{1}{4} \\times \\frac{1}{4} = \\frac{1}{16}$) and so on. This hinders trying deeper networks. We refer to this phenomenon as **vaninishing gradient** problem.\n",
    "\n",
    "#### Other activation functions\n",
    "\n",
    "Nowadays, we mostly find ReLU or LeakyReLU being as they can cope with the vanishing gradient problem. In the graph below shows the ReLU activation that mathematically can be defined as $f(x) = \\text{max}(0,x)$. Consequently, the derivative of the function is 1 for values larger than zero. This prevents the gradient from vanishing if ReLU activations are applied repeatedly. \n",
    "\n",
    "<img src=\"imgs/relu.png\" width=\"300px\" />\n",
    "\n",
    "However, there still is another problem. Once we $x$ becomes less than 0, the gradient becomes 0 and we are able to recover from this state due to a lack of gradient information. We say that the activation got **stuck**. A solution to this can be to prevent constant outputs, in the case of the ReLU the solution is a leaky ReLU which has a slight slope in the negative region.\n",
    "\n",
    "<img src=\"imgs/leaky_relu.png\" width=\"300px\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93ceaca",
   "metadata": {},
   "source": [
    "### Batch Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3d258c",
   "metadata": {},
   "source": [
    "#### What is batch normalization and why do we use it?\n",
    "\n",
    "In the previous example, we already standardized the *input features* which ensures that the input distribution to have zero mean and unit standard deviation. <br/>\n",
    "Assuming that a linear layer is initialized with random weights of around 0, this, this ensures the linear layer's output is in the area of typical activation functions. This improves the adaptation rate as we avoid low/zero gradients. \n",
    "\n",
    "However, once networks get deeper, the chance increases that some linear layer in the network does map to a \"different area\", implying that the distribution at a particular layer isn't a *N(0,1)* distribution. This phenomenon is referred to as **internal covariate shift (ICS)**. In other words, the term \"internal covariance shift\" refers to a change of the output distribution due to a shift in the layer parameters. Since layers of a neural network are dependent on each other, the following layer depends on the output distribution of the previous layer. This has an impact on learning.\n",
    "\n",
    "The idea of **Batch normalization** is simply to reduce this covariate shift by normalizing the output of intermediate layers. This has an impact on learning.\n",
    "\n",
    "Batch normalization *usually* has the following benefits:\n",
    "- Allows for higher learning rates as it prevents fast deterioration to low/zero\n",
    "gradients\n",
    "- Makes sigmoid-type activation functions more usable (for the same reason)\n",
    "- Has regularization properties which can sometimes have similar effects as dropout."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5a754a",
   "metadata": {},
   "source": [
    "#### How to apply batch normalization\n",
    "\n",
    "As mentioned previously, we would like to standardize the output of an intermediate layer in the network. Consequently, for a layer with $d$-dimensional input $x = (x^{(1)}, ..., x^{(n)})$, we will normalize each dimension such that\n",
    "\n",
    "$$\\hat{x^{(k)}} = \\frac{x^{(k)} - \\text{E}[x]}{{\\text{Var}[x]}}$$\n",
    "\n",
    "where the expectation and variance are computer over the training dataset. \n",
    "\n",
    "However, note that **simply normalizing each input of a layer may change what layer can represent**. For instance, let's assume that we have a linear layer followed by batch normalization, and we want it to learn the identity mapping. It turns out that due batch normalization, such mapping can only be learned if the expectation is 0 and variance is 1. \n",
    "\n",
    "However, to work around this issue, we perform an additional transformation directly after the normalization step. For the activation $\\hat{x^{(k)}}$, a learned pair of parameters $\\lambda^{(k)}$ and $\\beta^{(k)}$, which scales and shifts the normalized value.\n",
    "\n",
    "$$y^{(k)} = \\lambda^{(k)} \\hat{x^{(k)}} + \\beta^{(k)}$$\n",
    "\n",
    "These parameters are learned along with the original model, and restore the presentative power of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722cf63b",
   "metadata": {},
   "source": [
    "#### Batch normalization with mini-batch statistics\n",
    "\n",
    "Unfortunately, in practice, computing the expection and variance across the entire dataset is impractical. Therefore, a simplification is made: Since we use mini-batches in stochastic gradient training, each mini-batch produces estimates of the mean and variance of each activation. We can use this statistics to estimate the expectation and variance for the entire dataset.\n",
    "\n",
    "**During training**, batch normalization for a given batch $B = \\{x_1, ..., x_m \\}$ where the parameters $\\lambda$ and $\\beta$ are learned is implemented as follows:\n",
    "\n",
    "$$\\mu_B = \\frac{1}{m} \\sum_{i=1}^m x_i$$\n",
    "\n",
    "$$\\sigma^2_B = \\frac{1}{m} \\sum_{i=1}^m (x_i-\\mu_B)^2$$\n",
    "\n",
    "$$\\hat{x_i} = \\frac{x_i - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}} $$\n",
    "\n",
    "$$y^{(k)} = \\lambda \\hat{x^{(k)}} + \\beta$$\n",
    "\n",
    "**During inference**, the learned parameters $\\lambda$ and $\\beta$ are used to normalize the distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69448b7",
   "metadata": {},
   "source": [
    "### Basic imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a127b986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import trange\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2603675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To ensure reproducability\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5954d4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70015490",
   "metadata": {},
   "source": [
    "## Fetch the california housing dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f82cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = fetch_california_housing(as_frame=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c083760",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df = dataset['data']\n",
    "target_df = dataset['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dbddf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert the housing prices in the housing df\n",
    "housing_df['HousePrice'] = target_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56665718",
   "metadata": {},
   "source": [
    "## Prepare the training and testing set\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b4cbdc",
   "metadata": {},
   "source": [
    "### Splitting the dataframe (using sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d051252",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(housing_df, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9518d555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Briefly check whether we have the correct set sizes\n",
    "print('Test ratio: ', len(test_df) / (len(train_df) + len(test_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3bebe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_columns = ['MedInc', 'HouseAge', 'AveRooms', 'AveBedrms', 'Population', 'AveOccup', 'Latitude', 'Longitude']\n",
    "target_column = 'HousePrice'\n",
    "\n",
    "x_train = train_df[feature_columns].values\n",
    "y_train = train_df[['HousePrice']].values\n",
    "\n",
    "x_test = test_df[feature_columns].values\n",
    "y_test = test_df[['HousePrice']].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ededd5ea",
   "metadata": {},
   "source": [
    "### Extending the network\n",
    "\n",
    "We will now try to improve the network architecture that was introduced in the previous tutorial. <br/>\n",
    "More precisely, we will make the following adjustments to our network:\n",
    "\n",
    "- Replace the sigmoid with ReLU (or LeakyReLU) activations\n",
    "- Add BatchNormalization layers\n",
    "- Increase the number of parameters / layers\n",
    "\n",
    "The final structure of our network should look as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ba2c29",
   "metadata": {},
   "source": [
    "<img src=\"imgs/linear_model_2.png\" width=\"1000px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a66bf0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38735769",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dims = len(feature_columns)\n",
    "output_dims = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e230c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Sequential(\n",
    "    nn.Linear(input_dims, 512, bias=False),\n",
    "    nn.BatchNorm1d(512),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(512, 256, bias=False),\n",
    "    nn.BatchNorm1d(256),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(256, 256, bias=False),\n",
    "    nn.BatchNorm1d(256),\n",
    "    nn.LeakyReLU(),\n",
    "    nn.Linear(256, 128, bias=False),\n",
    "    nn.BatchNorm1d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0919efe5",
   "metadata": {},
   "source": [
    "We can now print the network to verify the network architecture is correct ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9bee96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8034dea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = torch.rand((64, 8))\n",
    "pred = model(x)\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b9058d",
   "metadata": {},
   "source": [
    "### Preprocess the samples "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7553ab8",
   "metadata": {},
   "source": [
    "Before training the neural network, input features should be normalized (very often to the range -1 to 1) or standardized, and converted to a PyTorch tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c787e717",
   "metadata": {},
   "source": [
    "#### Standardize the input features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e976c3b3",
   "metadata": {},
   "source": [
    "In this example, we will use Scikit's StandardScaler to standardized the input features. In future examples, we will start to use PyTorch's \"on-board capabilities\" to preprecess the input features (e.g., https://pytorch.org/vision/main/generated/torchvision.transforms.Normalize.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daf0e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(x_train)\n",
    "x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ac7243a",
   "metadata": {},
   "source": [
    "#### Convert all samples to a PyTorch tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cbaccbb",
   "metadata": {},
   "source": [
    "The entire dataset (`x_train` and `x_test`) is stored as numpy array at this point and we still have convert it to a PyTorch tensor.\n",
    "\n",
    "Tensors are similar to numpy ndarrays, except that tensors can run on GPUs or other hardware accelerators. In fact, tensors and NumPy arrays can often share the same underlying memory, eliminating the need to copy data (see Bridge with NumPy). Tensors are also optimized for automatic differentiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58fb2e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be63e23",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.tensor(x_train, dtype=torch.float32)\n",
    "x_test = torch.tensor(x_test, dtype=torch.float32)\n",
    "\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01792eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_train.shape)\n",
    "print(type(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4f5d3f",
   "metadata": {},
   "source": [
    "### Train the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae450929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix seed to ensure reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27642787",
   "metadata": {},
   "source": [
    "#### Helper function that computes for RMSE for a given dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242d024e",
   "metadata": {},
   "source": [
    "The *evaluate_model_performance()* function receives a set of features and their target levels as its input, and evaluates the model performance using the root mean square error. In order to do this, the network must be switched to *inference mode* (`model.eval()`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e220a7bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_performance(x, y, batch_size=128):\n",
    "    \n",
    "    assert x.shape[0] == y.shape[0], 'Feature and target labels a different number of samples'\n",
    "    \n",
    "    # During training, the batch norm layer keeps a running estimate of its computed mean and variance. \n",
    "    # At inference time, the mean/variance must not be updated. The estimed mean/variance is used for normalization.\n",
    "    model.eval()\n",
    "    \n",
    "    total_error = 0.\n",
    "    \n",
    "    num_samples = x.shape[0]\n",
    "    \n",
    "    # No gradient have to be computed at inference time. We can disable gradient computation by means of the\n",
    "    # no_grad() context manager. This will reduce the memory consumption.\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for indices in batch(range(num_samples), batch_size):\n",
    "            \n",
    "            # Convert the sample to a Pytorch tensor\n",
    "            sample_x = x[indices]\n",
    "            sample_y = y[indices]\n",
    "            \n",
    "            pred = model(sample_x)\n",
    "            \n",
    "            total_error += torch.sum((pred - sample_y) **2)\n",
    "            \n",
    "    rmse = torch.sqrt(total_error / num_samples) * 100000\n",
    "    \n",
    "    # Switch the network back to training mode since want to continue training.\n",
    "    model.train()\n",
    " \n",
    "    return rmse.numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cacde1e",
   "metadata": {},
   "source": [
    "#### Start training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60921ad",
   "metadata": {},
   "source": [
    "The training procedure for a neural network looks as follows: First, sample a random set of samples from the training set. This randomly drawn subset is referred to as a **batch**.\n",
    "\n",
    "The training batch is fed through the network to obtain the prediction (output of the network). Next, we need to update the parameters (weights) of the network. To accomplish this, we first have to compute the loss (a.k.a. cost function) and then compute the gradient with respect to every parameter in the network. Once we obtain the individual gradients, we can update the parameters. For updating the parameters, we use an optimizer such as ADAM or SGD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e82d4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a195986c",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "num_epochs = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c953c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use the ADAM optimizer to find the \"optimal\" network weights.\n",
    "# The ADAM optimization is a stochastic gradient descent method that is based on \n",
    "# adaptive estimation of first-order and second-order moments.\n",
    "optimizer = Adam(model.parameters(), lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9185ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rmse = []\n",
    "\n",
    "num_samples = x_train.shape[0]\n",
    "\n",
    "for epoch in trange(num_epochs):\n",
    "    \n",
    "    # Samples should be drawn in random order.\n",
    "    # Hence, we create a list of indices (in random order) from which we can sample from.\n",
    "    shuffled_indices = list(range(num_samples))\n",
    "    random.shuffle(shuffled_indices)\n",
    "    \n",
    "    batch_iter = batch(shuffled_indices, 128)\n",
    "   \n",
    "    for batch_idx, sample_indices in enumerate(batch_iter):\n",
    "  \n",
    "        # Clear all gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch_x = x_train[sample_indices]\n",
    "        batch_y = y_train[sample_indices]\n",
    "\n",
    "        # Push the training batch through the neural network\n",
    "        pred = model(batch_x)\n",
    "\n",
    "        # Compute the mean squared error --> Our goal is to minimize this error\n",
    "        loss = F.mse_loss(pred, batch_y)\n",
    "\n",
    "        # Trigger backpropagation. This computes all gradients that have to be optimized.\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the weights based on the obtained gradient values\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % 50 == 0:\n",
    "            train_rmse.append(evaluate_model_performance(x_train, y_train))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d664d7ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(list(range(len(train_rmse))), train_rmse)\n",
    "plt.title('Train RMSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c5da03",
   "metadata": {},
   "source": [
    "### Performance evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe35647",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_rmse = evaluate_model_performance(x_train, y_train)\n",
    "test_rmse = evaluate_model_performance(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85938a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Train RMSE:', train_rmse)\n",
    "print('Test RMSE:', test_rmse)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
